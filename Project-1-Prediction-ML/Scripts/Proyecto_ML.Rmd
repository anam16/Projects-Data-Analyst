---
title: "Proyecto final Machine Learning "
subtittle: "Máster en Bioinformática"
author: "Ana Maria Muñoz Morales (munozm.anamaria@gmail.com)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El cáncer de mama es el tipo de cáncer más común.Este se origina en las células del revestimiento (epitelio) de los conductos (85%) o lóbulos (15%) del tejido glandular de los senos. Al comienzo, el tumor canceroso está confinado en el conducto o lóbulo (in situ), donde generalmente no causa síntomas y tiene un mínimo potencial de diseminación (metástasis). Con el paso del tiempo, este cáncer in situ (estadio 0) puede progresar e invadir el tejido mamario circundante (cáncer de mama invasivo), y a continuación propagarse a los ganglios linfáticos cercanos (metástasis regional) u a otros órganos del organismo (metástasis distante).

La respuesta al tratamiento está influenciada por la composición del ecosistema tumoral y las interacciones dentro de él.En esta práctica se va a realizar un análisis de datos provenientes de pacientes con cáncer de mama, incluyendo datos clínicos, de sistema inmune, moleculares y de análisis de patología digital. El objetivo es desarrollar un modelo para predecir si una paciente va a responder satisfactoriamente al tratamiento neoadyuvante previo a la cirugía para extirpar el tumor. Para ello, se utilizará el paper "Multi-omic machine learning predictor of breast cancer therapy response" como base, el cual utiliza la integración de datos multiómicos y el aprendizaje automático para generar modelos predictivos de respuesta al tratamiento. Se utilizarán cuatro ficheros de datos que contienen información clínica, de análisis digital, del sistema inmune y transcriptómica, los cuales se analizarán de forma independiente y combinada para identificar las variables más importantes en la respuesta al tratamiento.

# Análisis a realizar

A continuación, se detalla la lista de análisis que se pide realizar sobre el conjunto de datos

## Análisis exploratorio 


Para poder responder a las preguntas propuestas en este trabajo se procede a cargar las siguientes librerias:

```{r}
library(dplyr)
library(tidyr)
library(corrplot)
library(pROC)
library(ggplot2)
library(lattice)
library(nortest)
library(caret)
library(ggcorrplot)
library(graphics)
library(RColorBrewer)
library(stats)
library(ggplot2)
library(ranger)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
library(mlbench)
library(tensorflow)
library(keras)
library(knitr)
```

Una vez que las librerías estan cargadas, se procede a la resolución de la práctica.

### Pregunta 1

**En cada uno de los cuatro ficheros, ¿cuántos predictores hay? ¿Y cuántas muestras? ¿Qué identificador tiene la columna correspondiente al identificador de paciente? ¿Cuántos pacientes hay que tengan datos disponibles en los cuatro ficheros?**

Para responder a cada una de estas preguntas, se procede a leer cada uno de los ficheros proporcionados.

1.  El **fichero Main.csv**, contiene los datos de un análisis clásico de tumores. Este conjunto de datos se llamará **datos_clínicos**.

```{r}
datos_clinicos <- read.csv("/home/alumno25/ML/Main.csv",stringsAsFactors = F, header=T, sep = ",")
datos_clinicos_copy <- datos_clinicos
```

Este código sirve para leer el archivo csv denominado Main, de la ruta específicada, empleando la función `read.csv()`. Con la opción `stringsAsFactors = F`, se le indica a read.csv() que no se debe convertir las cadenas en factores.La opción `header = T`, sirve para indicar a read.csv() que el archivo CSV contiene una fila de encabezados que describen el nombre de las columnas. Por último, en este código con la opción `sep = ","` indica que los datos en el archivo CSV están separados por comas.

-   Para saber el número de **predictores** que hay en el fichero Main.csv, se procede a contar el número de columnas que hay en el dataset denominado datos_clinicos con la función `ncol()`. El menos 2 se pone para excluir las columnas "X" y "Trial.ID"ya que, estas no proporcionan información predictiva, sino que solo informan del número de muestra ("X") y del identificador único para cada paciente("Trial.ID").

```{r}
ncol(datos_clinicos) - 2
```

**Resultado:** El número de predictores en el fichero Main.csv es de 72.

-   Para saber cuantas **muestras** hay en fichero Main, se procede a contar el número de filas del dataset datos_clinicos con la función `nrow()` para obtener el número de muestras que contiene.

```{r}
nrow(datos_clinicos)
```

**Resultado:** El número de muestras en el fichero Main.csv es de 147.

-   Para determinar el **identificador que tiene la columna correspondiente al identificador de cada paciente** , se procede a buscar el nombre que tiene la columna 2 con la función `colnames()`, la cual es donde se encuentran los identificadores de los pacientes.

```{r}
colnames(datos_clinicos)[2]
```

**Resultado:** El identificador de la columna del fichero Main.csv es Trial.ID.

2.  El **fichero DigPathology.tsv**, contiene los detalles sobre el análisis digital de la imagen del tumor. Este conjunto de datos se llamará **analisis_digital**.

```{r}
analisis_digital <- read.delim("/home/alumno25/ML/DigPathology.tsv",stringsAsFactors = F, sep = "\t")
analisis_digital_copy <- analisis_digital
```

Para leer el archivo tsv denominado DigPathology de la ruta especificada, se usa la función `read.delim()`. Con la opción `stringsAsFactors = F`, se le indica a read.delim() que no debe convertir las cadenas en factores y con la opción `sep = ","` se indica que los datos en el archivo están separados por espacios.

-   Para saber el **número de predictores** correspondientes al fichero DigPathology.tsv se procede a contar el número de columnas que hay en el dataset denominado analisis_digital con la función `ncol()`. El menos 1 se pone para excluir la columna de identificación de cada paciente ya que, esta no proporciona información predictiva, sino que es solo un identificador único para cada paciente.

```{r}
ncol(analisis_digital) - 1
```

**Resultado:** El número de predictores que hay en el fichero DigPathology es de 7.

-   Para saber cuantas **muestras** hay en fichero DigPathology , se procede a contar el número de filas del dataset analisis_digital con la función `nrow()`y asi obtener el número de muestras.

```{r}
nrow(analisis_digital)
```

**Resultado:** El número de muestras en el fichero DigPathology es de 166.

-   Para determinar el **identificador que tiene la columna correspondiente al identificador de cada paciente**, se procede a buscar el nombre que tiene la columna 1, la cual es donde se encuentran los identificadores de los pacientes.

```{r}
colnames(analisis_digital)[1]
```

**Resultado:**El identificador de la columna correspondiente al identificador de cada paciente es Trial.ID.

3.  El **fichero mutational-signatures.tsv**, contiene detalles sobre el sistema inmune de las pacientes. Este conjunto de datos se llamará **sistema_inmune**.

```{r}
sistema_inmune <- read.delim("/home/alumno25/ML/mutational-signatures.tsv",stringsAsFactors = F, sep = "\t")
sistema_inmune_copy <- sistema_inmune
```

El código usado para leer este archivo es igual al del fichero DigPathology.tsv.

-   Para saber cuantos **predictores** hay en el fichero mutational-signatures.tsv se procede a contar el número de columnas que hay en el dataset denominado sistema_inmune con la función `ncol()`. El menos 1 se pone para excluir la columna de identificación de cada paciente ya que, esta no proporciona información predictiva, sino que es solo un identificador único para cada paciente.

```{r}
ncol(sistema_inmune) - 1
```

**Resultado:** El número de predictores en el fichero mutational-signatures.tsv es de 32.

-   Para saber cuantas **muestras** hay en fichero mutational-signatures.tsv , se procede a contar el número de filas del dataset sistema_inmune para obtener el número de muestras con la función `nrow()`.

```{r}
nrow(sistema_inmune)
```

**Resultado:** EL número de muestras del fichero mutational-signatures es de 163.

-   Para determinar el **identificador que tiene la columna correspondiente al identificador de cada paciente**,se procede a buscar el nombre que tiene la columna 1 con la función `colnames()`, la cual es donde se encuentran los identificadores de los pacientes.

```{r}
colnames(sistema_inmune)[1]
```

**Resultado:** El identificador de la columna correspondiente al identificador de cada paciente es X.

4.  El **fichero RNAseq-rawcounts.tsv**, contiene información sobre la cuantificación de genes relevantes para el estudio sin normalizar, en forma de un conteo de las reads obtenidas de los ensayos RNA-seq. Este conjunto de datos se llamará transcriptómica.

```{r}
transcriptomica <- read.delim("/home/alumno25/ML/RNAseq-rawcounts.tsv",stringsAsFactors = F, sep = "\t")
```

El código usado para leer este archivo es igual al del fichero DigPathology.tsv.

Para poder utilizar este conjunto de datos en pasos posteriores hay que transponer el fichero transcriptómica para que, cada fila represente un paciente y cada columna represente un gen. De esta manera, el formato será consistente con los otros conjuntos de datos y se podrá combinar para entrenar los distintos modelos.Para realizar dicho paso, se procede:

```{r}
transcriptomica_copy <- transcriptomica
transcriptomica_copy$X <- NULL
transcriptomica_t <- t(transcriptomica_copy)
gene_names <- transcriptomica$X
colnames(transcriptomica_t) <- gene_names
transcriptomica_t <- data.frame(Trial.ID = rownames(transcriptomica_t), transcriptomica_t)
```

Con el código anterior se crea una copia del objeto transcriptomica.A continuación, de la copia se elimina la columna "X" y se procede a trasponer filas por columnas con la función `t()`.Por último,se crea un vector llamado gene_names con los nombres de las columnas originales de transcriptomica para posteriormente crear un dataframe con la fución `data.frame()`, agregando una nueva columna llamada "Trial.ID" que contiene los nombres de fila originales de transcriptomica_t.

-   Para saber cuantos **predictores** hay en el fichero RNAseq-rawcounts.tsv, se procede a contar el número de columnas que hay en el dataset denominado transcriptómica_t con la función `ncol()`. El menos 1 se pone para excluir la columna de identificación de paciente ya que, esta no proporciona información predictiva, sino que es solo un identificador único para cada paciente.

```{r}
ncol(transcriptomica_t) - 1
```

**Resultado:** El número de predictores del fichero RNA-seqcounts.tsv es de 57905.

-   Para saber cuantas **muestras** hay en fichero RNAseq-rawcounts.tsv,se procede a contar el número de filas del dataset transcriptomica con la función `nrow()`y asi obtener el número de muestras.

```{r}
nrow(transcriptomica_t)
```

**Resultado**: El número de muestras en el fichero RNAseq-rawcounts.tsv es de 162.

-   Para determinar el **identificador que tiene la columna correspondiente al identificador de cada paciente** ,se procede a buscar el nombre que tiene la columna 1 con la función `colnames()`, la cual es donde se encuentran los identificadores de los pacientes

```{r}
colnames(transcriptomica_t)[1]
```

**Resultado:** El identificador de la columna correspondiente de cada paciente en el fichero RNAseq-rawcounts.tsv es Trial.ID.

Para responder a la pregunta de **¿Cuántos pacientes hay que tengan datos disponibles en los cuatro ficheros?**, se procede a ejecutar el siguiente código:

```{r}
id_DC <- datos_clinicos$Trial.ID
id_AD <- analisis_digital$Trial.ID
id_S <- sistema_inmune$X
id_T <- transcriptomica_t$Trial.ID
id_common <- intersect(id_DC,id_AD)
id_common2 <- intersect(id_common,id_S)
id_common3 <- intersect(id_common2,id_T)
length(id_common3)
```

Con este código lo que se hace es generar variables donde cada una almacena:

-   La **variable id_DC**, se le asignan los valores de la columna Trial.ID del dataset datos_clinicos que corresponden con los identificadores de los pacientes.
-   La **variable id_AD**, se le asignan los valores de la columna Trial.ID del dataset analisis_digital que corresponden con los identificadores de los pacientes.
-   La **variable id_S**, se le asignan los valores de la columna X del dataset sistema_inmune que corresponde con los identificadores de los pacientes.
-   La **variable id_T** se le asignan los valores de la columna Trial.ID del dataset transcriptomica_t que correspoden con los identificaodres de los pacientes.
-   La función `intersect()`, se usa para comprobar los identificadores que coinciden en los distintos dataset y la función `length ()` se usa para saber cual es el número de identificadores que se repiten en todos los datasets, **siendo 143 pacientes que tienen datos en los 4 datasets**.

### Pregunta 2

**¿Hay valores nulos en alguno de los ficheros? ¿Cómo los tratarías?**

Para saber si hay valores nulos en alguno de los ficheros se procede a ejecutar el siguiente código:

```{r}
DC <- any(is.na(datos_clinicos))
AD <- any(is.na(analisis_digital))
SI <- any(is.na(sistema_inmune))
Tr <- any(is.na(transcriptomica_t))
print(paste(DC,AD,SI,Tr))
```

Con el código anterior lo que se hace es comprobar gracias al uso de la función `is.na()` si hay algun valor nulo en los datasets empleado debido a que, esta devolverá TRUE en el caso de que haya algun valor nulo en los datos y FALSE en el caso de que no haya ninguno. **Resultado:**No hay valores nulos en ninguno de los 4 ficheros.

### Pregunta 3

**Describe, para cada fichero, el número de variables numéricas (ya sean enteras o reales) y el número de variables factor (categóricas) sin enumerarlas.**

Para el fichero **Main.csv**, se procede a usar la funciones `is.numeric()`, `is.factor()` e `is.character()` para comprobar el tipo de variables numéricas, categóricas y de tipo carácter que hay en el dataframe denominado datos_clinicos. También se usa la función `sum()` para saber el total de cada una de este tipo de variables.

```{r}
num_vars <- sum(sapply(datos_clinicos, is.numeric))
cat_vars <- sum(sapply(datos_clinicos, is.factor))
num_vars_caracteres <- sum(sapply(datos_clinicos, is.character))

cat("El archivo Main.csv tiene", num_vars, "variables numéricas, ", cat_vars, "variables categóricas y ",num_vars_caracteres, "variable de tipo caracter")
```

Tras realizar este paso se observa que todas, a excepción de una variable, son numéricas.Por tanto, se obtiene la información de que el conjunto de datos **datos_clinicos**, a priori, se encuentran varias variables donde el tipo de dato no se corresponde con la naturaleza del valor que contiene, como es el caso de:

-   `resp.pCR`: variable que identifica si el paciente ha tenido una respuesta patológica completa.
-   `resp.Chemoresistant`:variable que identifica si los pacientes van a tener un tumor que será quimiorresistente.
-   `resp.Chemosensitive` variable que identifica si el tumor del paciente es quimiosensible o no.
-   `RCB.category`: variable que clasifica la carga de cancer residual (RCB) al finalizar el tratamiento neoadyudante.
-   `Histology`: variable que identifica el tipo de histología del tumor. El grado histológico es una evaluación microscópica de la apariencia de las células cancerosas y su nivel de diferenciación en comparación con las células normales del tejido de origen.
-   `ER.status`:variable que identifica el estado del receptor de estrógeno.
-   `HER2.status`:variable que identifica el estado del receptor de HER2(receptor del factor de crecimiento epidérmico humano 2).
-   `LN.at.diagnosis`:variable que identifica la cantidad de ganglios linfáticos afectados en el momento del diagnóstico.
-   `Grade.pre.chemotherapy`: variable que identifica el grado del tumor antes de la quimioterapia.
-   `CodingMuts.PIK3CA`: variable que identifica si hay mutaciones en el gen PIK3CA.
-   `CodingMuts.TP53`:variable que identifica si hay mutaciones en el gen TP53.
-   `HLA.LOH`: variable que identifica la pérdida de heterocigosidad.
-   Las variables `Chemo.first.Taxane`, `Chemo.first.Anthracycline`, `Chemo.second.Taxane`, `Chemo.second.Anthracycline`, `Chemo.any.Anthracycline` y `Chemo.any.antiHER2` representan la presencia o ausencia de un tipo particular de tratamiento.

Con el siguiente código se elimina la columna X ya que,solo nos dice el número de muestra y no es relevante en este estudio y se transforman las variables numéricas mencionadas anteriormente a categóricas.

```{r}
datos_clinicos_copy$X=NULL
datos_clinicos_copy[, c("resp.pCR", "resp.Chemoresistant","resp.Chemosensitive","RCB.category","Histology","ER.status","HER2.status","LN.at.diagnosis","Grade.pre.chemotherapy","CodingMuts.PIK3CA","CodingMuts.TP53","HLA.LOH","Chemo.first.Taxane", "Chemo.first.Anthracycline", "Chemo.second.Taxane", "Chemo.second.Anthracycline", "Chemo.any.Anthracycline","Chemo.any.antiHER2")] <- lapply(datos_clinicos[, c("resp.pCR", "resp.Chemoresistant","resp.Chemosensitive","RCB.category","Histology","ER.status","HER2.status","LN.at.diagnosis","Grade.pre.chemotherapy","CodingMuts.PIK3CA","CodingMuts.TP53","HLA.LOH","Chemo.first.Taxane", "Chemo.first.Anthracycline", "Chemo.second.Taxane", "Chemo.second.Anthracycline", "Chemo.any.Anthracycline","Chemo.any.antiHER2")], as.factor)
```

Por tanto, al repetir el código anterior:

```{r}
num_vars <- sum(sapply(datos_clinicos_copy, is.numeric))
cat_vars <- sum(sapply(datos_clinicos_copy, is.factor))
num_vars_caracteres <- sum(sapply(datos_clinicos_copy, is.character))

cat("El archivo Main.csv tiene", num_vars, "variables numéricas, ", cat_vars, "variables categóricas y ",num_vars_caracteres, "variable de tipo caracter.")
```

Se obtiene como resultado que el fichero denominado **Main.csv** contiene 54 variables numéricas, 18 de tipo categórico y 1 de tipo caracter.

Para el fichero **mutational-signatures.tsv**, se procede a usar la funciones `is.numeric()`, `is.factor()` e `is.character()` para comprobar el tipo de variables numéricas, categóricas y de tipo carácter que hay en el dataframe denominado sistema_inmune. También se usa la función `sum()` para saber el total de cada una de este tipo de variables.

```{r}
num_vars2 <- sum(sapply(sistema_inmune, is.numeric))
cat_vars2 <- sum(sapply(sistema_inmune, is.factor))
num_vars_caracteres2 <- sum(sapply(sistema_inmune, is.character))

cat("El archivo mutational-signatures.tsv  tiene", num_vars2, "variables numéricas, ", cat_vars2, "variables categóricas y ",num_vars_caracteres2, "variable de tipo caracter.")
```

Se obtiene como resultado que el fichero denominado **mutational-signatures.tsv** contiene 32 variables numéricas, 0 de tipo categórico y 1 de tipo caracter.

Para el fichero denominado **DigPathology.csv** , se procede a usar la funciones `is.numeric()`, `is.factor()` e `is.character()` para comprobar el tipo de variables numéricas, categóricas y de tipo carácter que hay en el dataset denominado **analisis_digital**. También se usa la función `sum()` para saber el total de cada una de este tipo de variables.

```{r}
num_vars3 <- sum(sapply(analisis_digital, is.numeric))
cat_vars3 <- sum(sapply(analisis_digital, is.factor))
num_vars_caracteres3 <- sum(sapply(analisis_digital, is.character))

cat("El archivo DigPathology tiene", num_vars3, "variables numéricas, ", cat_vars3, "variables categóricas y ",num_vars_caracteres3, "variable de tipo caracter.")
```

Se obtiene como resultado del fichero denominado **DigPathology.csv** contiene 7 variables numéricas, 0 de tipo categórico y 1 de tipo caracter.

Para el fichero denominado **RNAseq-rawcounts.tsv** , se procede a usar la funciones `is.numeric()`, `is.factor()` e `is.character()` para comprobar el tipo de variables numéricas, categóricas y de tipo carácter que hay en el dataset denominado **transcriptómica_t**. También se usa la función `sum()` para saber el total de cada una de este tipo de variables.

```{r}
num_vars4 <- sum(sapply(transcriptomica_t, is.numeric))
cat_vars4 <- sum(sapply(transcriptomica_t, is.factor))
num_vars_caracteres4 <- sum(sapply(transcriptomica_t, is.character))

cat("El archivo RNAseq-rawcounts.tsv tiene", num_vars4, "variables numéricas, ", cat_vars4, "variables categóricas y ",num_vars_caracteres4, "variable de tipo caracter.")
```

Se obtiene como resultado del fichero denominado **RNAseq-rawcounts.tsv** contiene 57905 variables numéricas, 0 de tipo categórico y 1 de tipo caracter.

### Pregunta 4

**¿Están normalizadas las variables en cada uno de los cuatro ficheros? En este punto del análisis, ¿es necesario normalizarlas?**

Para saber si las variables de los ficheros están normalizadas se usa la prueba de **Test de Kolmogorov-Smirnov** (con la corrección Lilliefors).Esta se utiliza para contrastar si un conjunto de datos se ajustan o no a una distribución normal. Es similar en este caso al test de Shapiro Wilk, pero la principal diferencia con éste radica en el número de muestras. Mientras que el test de Shapiro Wilk se puede utilizar con hasta 50 datos, el test de Kolmogorov Smirnov es recomendable utilizarlo con más de 50 observaciones. El contraste de esta hipótesis es: - Hipótesis nula: la muestra proviene de una distribución normal.

-   Hipótesis alternativa: la muestra no tiene una distribución normal.

Para realizar esto se elige un nivel de significancia de 0.05, entonces si el valor p es menor que 0.05, significa que hay evidencia suficiente para rechazar la hipótesis nula de que los datos tienen una distribución normal. Mientras que, si el valor p es mayor que 0.05, significa que no hay evidencia suficiente para rechazar la hipótesis nula de que los datos tienen una distribución normal.

En el código usado se observa, que se realiza una prueba de normalidad en cada columna numérica de cada uno de los conjuntos de datos (datos_clinicos_copy,sistema_inmune, analisis_digital) utilizando el Test de Kolmogorov-Smirnov(con la corrección Lilliefors). Además,se comprueba que la varianza de los datos numéricos de cada uno de los datasets sea distinta de 0 para poder aplicar este test ya que, estas pruebas asumen que las variables tienen una varianza finita y no nula. Si una variable tiene una varianza igual a cero, esto significa que todos los valores en esa variable son iguales, y no hay variabilidad para evaluar la normalidad.Por lo tanto, antes de aplicar cualquier prueba de normalidad, es importante comprobar si la varianza de cada variable es distinta de cero.La salida del código indica qué variables numéricas siguen una distribución normal y cuáles no.

Comprobar normalidad en el dataset **datos_clinicos_copy**

```{r}

    for (i in 1:ncol(datos_clinicos_copy)) {
      
      if (is.numeric(datos_clinicos_copy[,i])) {
        
        if (var(datos_clinicos_copy[,i]) != 0) {
          
          resultado <- lillie.test(datos_clinicos_copy[,i])
          
          cat(colnames(datos_clinicos_copy)[i], ": valor p = ", resultado$p.value, "\n")
         
          if (resultado$p.value < 0.05) {
            
            cat("La variable", colnames(datos_clinicos_copy)[i], "no sigue una distribución normal.\n")
          } else {
            
            cat("La variable", colnames(datos_clinicos_copy)[i], "sigue una distribución normal.\n")
          }
        }
      }
    }
```

**Resultado:** Tras analizar las variables del fichero Main.csv, se concluye que algunas variables están normalmente distribuidas y otras no, esto puede deberse a que las diferentes variables tienen diferentes distribuciones o que los datos tienen diferentes patrones de variabilidad.

Comprobar normalidad en el dataset **analisis_digital.**

```{r}

for (i in 1:ncol(analisis_digital)) {
  
  if (is.numeric(analisis_digital[,i])) {
    
    if (var(analisis_digital[,i]) != 0) {
      
      resultado <- lillie.test(analisis_digital[,i])
      
      cat(colnames(analisis_digital)[i], ": valor p = ", resultado$p.value, "\n")
     
      if (resultado$p.value < 0.05) {
        
        cat("La variable", colnames(analisis_digital)[i], "no sigue una distribución normal.\n")
      } else {
        
        cat("La variable", colnames(analisis_digital)[i], "sigue una distribución normal.\n")
      }
    }
  }
}
```

**Resultado:** Tras analizar las variables del fichero DigPathology.tsv, se concluye que las variables de este fichero no siguen una distribucción normal a excepción de la variable n_total.

Comprobar normalidad en el dataset **sistema_inmune.**

```{r}

for (i in 1:ncol(sistema_inmune)) {
  
  if (is.numeric(sistema_inmune[,i])) {
    
    if (var(sistema_inmune[,i]) != 0) {
     
      resultado <- lillie.test(sistema_inmune[,i])
      
      cat(colnames(sistema_inmune)[i], ": valor p = ", resultado$p.value, "\n")
      
      if (resultado$p.value < 0.05) {
        
        cat("La variable", colnames(sistema_inmune)[i], "no sigue una distribución normal.\n")
      } else {
        
        cat("La variable", colnames(sistema_inmune)[i], "sigue una distribución normal.\n")
      }
    }
  }
}
```

**Resultado:** Tras analizar las variables del fichero mutational-signatures.tsv, se concluye que no siguen una distribucción normal.

El fichero RNAseq-rawcounts.tsv, se sabe que no esta normalizado porque en la introducción de la descripción de esta práctica de los datos a usar nos da dicha información sobre este fichero.Por tanto, ninguno de los 4 ficheros esta normalizado.

En este punto del análisis, no es necesario normalizar las variables, pero es recomendable hacerlo ya que, en análisis posteriores, como el análisis de componentes principales, es importante tener datos normalizados para asegurar que todas las variables tengan la misma escala y evitar sesgos en la transformación. Además, normalizar los datos en esta etapa de preparación puede ser más útil debido a que evita la necesidad de hacerlo en análisis posteriores y permite tener los datos preparados para futuros análisis de manera más eficiente.

A pesar de que como se ha observado anteriormente hay alguna variables que siguen una distribucción normal,se procede a normalizarlas todas para asegurar que las diferentes variables tengan la misma escala y distribución. Para la normalización de las variables en los distintos dataset se usará la siguiente función:

```{r}
escalado <- function(dataset) {
  
  numeric_columns <- sapply(dataset, is.numeric)
  
  escalado <- scale(dataset[, numeric_columns])
  
  escalado <- as.data.frame(escalado)
  
  no_numericas <- dataset[, !numeric_columns, drop = FALSE]
  
  resultado <- cbind(escalado, no_numericas)
  
  return(resultado)
}

DC_escalado <- escalado(datos_clinicos_copy)
AD_escalado <- escalado(analisis_digital_copy)
SI_escalado <- escalado(sistema_inmune_copy)
```

Esta función llamada "escalado" se utiliza para estandarizar las variables numéricas de los distintos conjuntos de datos. En primer lugar, la función identifica las columnas numéricas del conjunto de datos mediante la función `sapply()`. A continuación, utiliza la función `scale()` para escalar y centrar las columnas numéricas en un rango de valores estándar, lo que ayuda a evitar la dominancia de algunas variables en comparación con otras durante el análisis. Luego, la función convierte el resultado en un objeto de data frame utilizando la función `as.data.frame()`. Posteriormente, la función separa las columnas no numéricas del conjunto de datos original y las une con las columnas escaladas. Finalmente, la función devuelve el conjunto de datos con las columnas escaladas y las no numéricas unidas.

\* Cabe destacar, que en esta proceso el fichero de transcriptómica no se ha escalado debido a que, su procedimiento es más complejo y se realizará posteriormente cuando se procese todo este fichero para poder trabajar con este en los distintos modelos.

Para comprobar si el proceso de escalar los valores de cada uno de los datasets se ha realizado correctamente se procede a comprobar su media y su desviación estandar puesto que si este proceso se ha realizado de manera correcta, la media de cada columna debe de tener valores muy cercanos a 0 o 0 y la desviación estandar valores muy cercanos a 1 o 1. Para verificar esto, se usa el siguiente código que lo que hace es utilizar la función `sapply()` para identificar las columnas numéricas de cada conjunto de datos. A continuación, utiliza la función `apply()` para calcular la media y la desviación estándar de cada columna numérica. Los resultados se almacenan en las variables **media_cols_numericas y sd_cols_numericas**, respectivamente. Luego, el código utiliza la función `data.frame()` para crear un nuevo data frame llamado **resultados**, que contiene las columnas "Media" y "Desviacion_estandar" con los resultados de los cálculos anteriores. Finalmente, el código utiliza la función print para mostrar el contenido del data frame resultados por consola.

Dataset **DC_escalado**, contiene la información de los datos clínicos:

```{r}

cols_numericas <- sapply(DC_escalado, is.numeric)
media_cols_numericas <- apply(DC_escalado[, cols_numericas], 2, mean)
sd_cols_numericas <- apply(DC_escalado[, cols_numericas], 2, sd)
resultados <- data.frame(Media = media_cols_numericas, 
                         Desviacion_estandar = sd_cols_numericas)
print(resultados)

```

Dataset **AD_escalado**, contiene la información del análisis digital:

```{r}

cols_numericas <- sapply(AD_escalado, is.numeric)
media_cols_numericas <- apply(AD_escalado[, cols_numericas], 2, mean)
sd_cols_numericas <- apply(AD_escalado[, cols_numericas], 2, sd)
resultados <- data.frame(Media = media_cols_numericas, 
                         Desviacion_estandar = sd_cols_numericas)

print(resultados)
```

Dataset **SI_escalado**, contiene la información del sistema inmune:

```{r}
cols_numericas <- sapply(SI_escalado, is.numeric)
media_cols_numericas <- apply(SI_escalado[, cols_numericas], 2, mean)
sd_cols_numericas <- apply(SI_escalado[, cols_numericas], 2, sd)
resultados <- data.frame(Media = media_cols_numericas, 
                         Desviacion_estandar = sd_cols_numericas)
print(resultados)
```

Tras comprobar en cada uno de los datasets escalados, se observa que el proceso se ha realizado correctamente ya que,se observa que la media de cada columna tiene valores muy cercanos a 0 o 0 y la desviación estandar valores muy cercanos a 1 o 1.

### Pregunta 4

**Las variables importantes son, sobre todo, las que nos dan información sobre la respuesta al tratamiento, a saber `resp.pCR`, `resp.Chemoresistant`, `resp.Chemosensitive`, `RCB.score` y `RCB.category`. Descríbelas según la información que aparece en el paper y descríbelas según su tipo. Si son numéricas, usa whisker plots o histogramas para representarlas y coméntalas. Si son categóricas, proporciona información sobre la distribución de sus valores.**

-   La **variable resp.pCR**, es una variable categórica que sirve para saber si el paciente ha tenido una respuesta patológica completa, es decir, si este tiene ausencia de todos los signos de cancer en el tejido después de aplicar el tratamiento neoadyudante.

```{r}
tabla_DC <-table(datos_clinicos_copy$resp.pCR)
tabla_DC
```

Al usar la función `table()` se puede observar que la variable **`resp.pCR`** del dataset **`datos_clinicos_copy`** tiene dos valores posibles: 0 y 1. El valor 0 representa el caso en que no hubo una respuesta patológica completa después de un tratamiento, y el valor 1 representa el caso en que sí la hubo. En esta salida se observa la frecuencia o el número de observaciones que tienen cada valor. En este caso, hay 109 observaciones con el valor 0 y 38 observaciones con el valor 1. Además, se puede comentar también que el valor 0 es mucho más frecuente que el valor 1, lo que significa que hay más observaciones sin una respuesta patológica completa que con ella.También podemos calcular la proporción de observaciones en cada categoría utilizando la función `prop.table()`.

```{r}
prop.table(tabla_DC)
```

Al realizar la probabilidad, se comprueba que como se mencionaba anteriormente hay más proporción de pacientes con un valor de 0 para la variables resp.pCR que con un valor 1.

Estos datos se puede ver de una manera más visual ejecutando el siguiente código:

```{r}
barplot(table(datos_clinicos_copy$resp.pCR),col="blue",ylim=c(0,125),yaxt = "n"); axis(side = 2, at = c(0, 25, 50, 75, 100,125));title("Variable resp.pCR")
```

Con esta gráfica se observa lo mencionado anteriormente de que la categoría 0 para la variable resp.pCR es más frecuente que la categoría 1, indicando esto que hay más casos donde no hay una respuesta patologicamente completa.

-   La **variable resp.Chemoresistant**, es una variable categórica que se refiere a aquellos pacientes que van a tener un tumor que será quimiorresistente.

```{r}
tabla_DC2 <- table(datos_clinicos_copy$resp.Chemoresistant)
tabla_DC2
```

Estos datos muestran la frecuencia de la variable **resp.Chemoresistant** . Esta variable indica si el tumor del paciente es quimiorresistente o no al tratamiento. El 0 significa que el tumor del paciente no es quimiorresistente y el 1 significa que sí lo es. Según estos datos, hay 122 pacientes que tienen un tumor que no es quimiorresistente y 25 que sí tienen un tumor quimoresistente. Además, se puede comentar también que el valor 0 es mucho más frecuente que el valor 1, lo que significa que hay más observaciones donde el tumor no es quimioresistente.También podemos calcular la proporción de observaciones en cada categoría utilizando la función `prop.table()`.

```{r}
prop.table(tabla_DC2)
```

Al realizar la probabilidad, se comprueba que como se mencionaba anteriormente hay más proporción de pacientes con un valor de 0 para la variable resp.Chemoresistant que con un valor 1.

Estos datos se puede ver de una manera más visual ejecutando el siguiente código:

```{r}
barplot(table(datos_clinicos_copy$resp.Chemoresistant),col="pink",ylim=c(0,125),yaxt = "n"); axis(side = 2, at = c(0, 25, 50, 75, 100,125));title("Variable resp.Chemoresistant")
```

Con esta gráfica se observa lo mencionado anteriormente de que la categoría 0 para la variable resp.Chemoresistant es más frecuente que la categoría 1, indicando esto que hay más casos donde el tumor no es quimioresistente.

-   La variable **resp.Chemosensitive** es una variable categórica que se refiere a si el tumor es quimiosensible responderá favorablemente a la quimioterapia, es decir, que este se reducirá o desaparecerá al ser tratado.

```{r}
tabla_DC3 <-table(datos_clinicos$resp.Chemosensitive)
tabla_DC3
```

Estos datos muestran la frecuencia de la variable **resp.Chemosensitive** . El 0 significa que el tumor del paciente no es quimiosensible y el 1 significa que sí lo es. Según estos datos, hay 86 pacientes que tienen un tumor que no es quimiosensible y 25 que sí tienen un tumor quimiosensible. Además, se puede comentar también que el valor 0 es más frecuente que el valor 1, lo que significa que hay más observaciones donde el tumor no es quimiosensible. También podemos calcular la proporción de observaciones en cada categoría utilizando la función `prop.table()`.

```{r}
prop.table(tabla_DC3)
```

Al realizar la probabilidad, se comprueba que como se mencionaba anteriormente hay más proporción de pacientes con un valor de 0 para la variable resp.Chemosensitive que con un valor 1 pero a diferencia de las anteriores la diferencia de proporción en esta para los dos grupos no es tan grande.

Estos datos se puede ver de una manera más visual ejecutando el siguiente código:

```{r}
barplot(table(datos_clinicos_copy$resp.Chemosensitive),col="orange",ylim=c(0,100),yaxt = "n"); axis(side = 2, at = c(0, 25, 50, 75, 100));title("Variable resp.Chemosensitive")
```

Con esta gráfica se observa lo mencionado anteriormente de que la categoría 0 para la variable resp.Chemosensitive es más frecuente que la categoría 1, indicando esto que hay más casos donde el tumor no es quimiosensitivo.

-   La **variable RCB.score**, es una variable numérica que se refiere a la puntuación para poder clasificar a los distintos tumores en tipo pCR, RCB-I,II o III.

```{r}
mediana_rcb <- median(datos_clinicos_copy$RCB.score)
bp <- boxplot(datos_clinicos_copy$RCB.score, main = "Variable RCB.score", ylab = "RCB.score", col = "aquamarine")
text(x = 1, y = mediana_rcb, labels = mediana_rcb, pos = 3, col = "red", cex = 1)

```

El boxplot es una herramientas de visualización para describir la distribución de los datos de una variable que en este caso se usa para describir la variable RCB.core.

En el **boxplot** :

-   La caja va a representar el 50% de los datos centrales, ya que el límite inferior de la caja marca la posición del primer cuartil, es decir, deja por debajo el 25% de los datos. Mientras que el límite superior de la caja representa el tercer cuartil, es decir, deja por debajo el 75% de los datos. En este caso, se observa que los valores de la caja se extienden desde 0 hasta 3, lo que indica que el 50% de los datos se encuentran dentro de ese rango.

-   La mediana(marcada con una línea horizontal) es el valor que nos divide la distribución de los datos en dos partes iguales, es decir, en el diagrama marcará el punto exacto donde la mitad de los valores está por encima de ese valor y la otra mitad por abajo. En este caso el valor de la mediana se encuentra en el valor 1.665.Como se observa en este diagrama la mediana no corta la caja por la mitad, indicando así que tenemos una asimetría ligeramente negativa porque la parte más larga es la inferior a la mediana.

-   A cada lado de la caja se dibujan líneas discontinuas que son los denominados bigotes que determina el límite para la detección de valores atípicos.En este caso solo nos encontramos con el bigote superior que se extiende del valor 3 al 4.Por último, se puede comentar que no se encuentran valores atípicos.

-   La **variable RCB.category**, es una variable categórica que se refiere a la categoria en la que se clasifica la carga de cancer residual (RCB) al finalizar el tratamiento neoadyudante, es decir, si es de clase pCR,RCB-I,II o III.

```{r}
tabla_DC4<- table(datos_clinicos_copy$RCB.category)
tabla_DC4
```

También podemos calcular la proporción de observaciones en cada categoría utilizando la función `prop.table()`.

```{r}
prop.table(tabla_DC4)
```

Estos datos muestran la frecuencia y probabilidad de la variable RCB.category , donde el valor:

-   **Valor 0**: indica que la paciente ha tenido una respuesta patológica completa después de aplicar el tratamiento neoadyudante.Esta tiene 38 casos con una probabilidad de 0.29, siendo la segunda más probable en este conjunto de datos.

-   **Valor 1**:indica que la paciente ha tenido una buena respuesta(RCB-I ) después de aplicar el tratamiento neoadyudante.Esta tiene 23 casos con una probabilidad de 0.16, siendo la tercera más probable en este conjunto de datos.

-   **Valor 2**: indica que la paciente ha tenido una respuesta moderada(RCB-II) después de aplicar el tratamiento neoadyudante.Esta tiene 61 casos con una probabilidad de 0.41, siendo la primera más probable en este conjunto de datos.

-   **Valor 3**: indica que la paciente ha tenido residuos extendidos de la gravedad (RCB-III) después de aplicar el tratamiento neoadyudante.Esta tiene 25 casos con una probabilidad de 0.17, siendo la menos probable en este conjunto de datos.

### Pregunta 5

**Mediante el uso de un plot del tipo `corrplot` analiza las posibles correlaciones entre las variables numéricas del fichero `Main.csv` y coméntalas.**

Para realizar este apartado se procede a realizar el siguiente código:

```{r}
DC_num_norm <- DC_escalado[,sapply(DC_escalado, is.numeric)]
DC_num_norm <- DC_num_norm[,]
DC_num_norm$RCB.score=NULL
correlation_DC <- cor(DC_num_norm)
variables_descartar <- findCorrelation(correlation_DC, cutoff = 0.8)
DC_escalado_filtrado <- DC_num_norm[,-variables_descartar]
```

Este código primero selecciona las columnas numéricas de un conjunto de datos llamado DC_escalado y las guarda en un nuevo conjunto de datos llamado DC_num_norm. Luego, elimina la columna RCB.score del nuevo conjunto de datos DC_num_norm debido a que esta es redundante en el análisis. A continuación, calcula la matriz de correlación, con la función `cor()`entre las variables numéricas restantes en el conjunto de datos DC_num_norm y almacena el resultado en correlation_DC. Posteriormente, encuentra las variables que están altamente correlacionadas (con un coeficiente de correlación superior a 0.8) y las guarda en variables_descartar. Finalmente, elimina estas variables altamente correlacionadas del conjunto de datos DC_num_norm utilizando el operador de indexación [,-variables_descartar] y guarda el resultado en DC_escalado_filtrado.\
Por último, con el siguiente código se calcula la matriz de correlación para las variables restantes en DC_escalado_filtrado y se guarda el resultado en una nueva variable llamada corr_DC2, esta última será la que se ploteará gracias a la implementación de la función `corrplot()`para observar la matriz de correlación de una manera más visual.

```{r}
corr_DC2 <- cor(DC_escalado_filtrado,method = "pearson")
col <- colorRampPalette(c("#BB4444","#EE9988","#FFFFFF","#77AADD","#4477AA"))
corrplot(corr_DC2,method = "shade",type="upper",order = "AOE",tl.cex = 0.5,tl.col = "black",col = col(200)[seq(1, 200, length.out = 5)],addshade = "all",diag=F)
```

La visualización generada muestra la matriz de correlación para aquellas variables que se han determinado que no estan altamente relacionadas del fichero Main.csv. La representación de la matriz de correlación muestra solo la mitad superior de la matriz de correlación, donde cada variable se compara con todas las demás.Esto se hace porque la matriz de correlación es simétrica, lo que significa que la correlación entre las variables x e y es igual a la correlación entre las variables y y x. Por lo tanto, no es necesario mostrar ambas mitades de la matriz, ya que serían exactamente iguales. Además, en esta matriz se observa que no se muestran los valores en la diagonal principal de la matriz, ya que son siempre 1 y no aportan información adicional.También se puede observar que gracias a la implementación del parámetro `order` en la función `corrplot()` las variables son ordenadas en función de la cantidad de variación que explican, de manera que las variables que explican más variación aparecen cerca del centro de la matriz y las que explican menos variación aparecen más alejadas. Por último,cabe añadir que los colores de los cuadrados representan la fuerza y la dirección de las correlaciones siendo, rojo para correlaciones negativas, azul para correlaciones positivas y blanco para correlaciones cercanas a cero. Cabe añadir que cuanto más oscuro es el color, más fuerte es la correlación.

Entre las variables correlacionadas de manera positiva y fuerte se encuentran: \* **TIDE.CAF y TIDE.Expression** podría explicarse por el hecho de que ambos son medidas de la respuesta inmune tumoral y están influenciados por factores similares en el microambiente tumoral, como la presencia de células inmunitarias específicas, la inflamación, la angiogénesis y la señalización celular.

-   **Expressed.NAg y All.TMB** podría deberse al hecho de que los antígenos tumorales mutantes (TMB) son reconocidos por el sistema inmunológico como extraños y, por lo tanto, pueden desencadenar una respuesta inmunitaria contra las células tumorales. Por lo tanto, un alto TMB podría resultar en una mayor expresión de antígenos en la superficie celular (NAg).

-   **HRD.TelomericAI y CIN.Prop** podría explicarse por el hecho de que ambas medidas son indicadores de inestabilidad genómica. La telomeric AI es una medida de la longitud de los telómeros, mientras que CINProp mide la proporción de células que experimentan cambios cromosómicos. La inestabilidad genómica puede ser un factor que contribuye a la progresión tumoral y la resistencia a la terapia.

-   **Danaher Neutrophilis y Danaher Macrophages** podría deberse al hecho de que ambas células inmunitarias están implicadas en la respuesta inflamatoria y pueden desempeñar un papel en la promoción o inhibición del crecimiento tumoral.

-   **Danaher Treg y Danaher NIK CD 56 dim.cells** podría explicarse por el hecho de que ambas células inmunitarias están implicadas en la regulación de la respuesta inmunitaria y pueden desempeñar un papel en la supresión de la respuesta inmunitaria contra las células tumorales.

-   **Danaher Treg y Danaher B cells o Danaher CD45** podría deberse a la interacción entre las células B y T en la respuesta inmunitaria y a la presencia de células inmunitarias específicas en el microambiente tumoral.

-   **Danaher NIK CD 56 dim.cells y Danaher B cells o Danaher CD45** también podría deberse a la interacción entre estas células inmunitarias en la respuesta inmunitaria y la presencia de células inmunitarias específicas en el microambiente tumoral.

Entre las variables correlacionadas de manera negativa y fuerte se encuentran:

-   **TIDE.TAM.M2 y Danaher Neutrophilis, Danaher Treg, y Danaher Macrophages** podría estar relacionada con su papel en la regulación del sistema inmunológico. TIDE.TAM.M2 está asociado con la presencia de macrófagos M2, que promueven la inmunosupresión y la tolerancia tumoral, mientras que la presencia de células inmunitarias como los neutrófilos, las células Treg y los macrófagos M1 están asociadas con una mayor respuesta inmunitaria antitumoral. Por lo tanto, la correlación negativa sugiere que una mayor presencia de macrófagos M2 está asociada con una menor presencia de neutrófilos, células Treg y macrófagos M1.

-   **TIDE.Exclusion y Danaher B-cells** sugiere que la presencia de células B puede estar asociada con una menor respuesta inmunitaria antitumoral.

-   **TIDE.MDSC y Danaher CD45** podría estar relacionada con la capacidad de las células mieloides supresoras (MDSC) para suprimir la respuesta inmunitaria antitumoral y su capacidad para reducir la expresión de CD45 en las células T. Esto sugiere que una mayor presencia de MDSC puede estar asociada con una menor expresión de CD45 en las células T, lo que a su vez puede estar relacionado con una menor respuesta inmunitaria antitumoral.

### Pregunta 6

**A la luz de un plot PCA de los dos primeros componentes principales de cada una de las pacientes, a partir de los datos clínicos del fichero `Main.csv`, ¿se aprecia cierta separabilidad entre las pacientes, según sus valores posibles de respuesta a la cirugía tras el tratamiento adyuvante? Desarrolla la respuesta ¿Qué variables serían las más relevantes en ambos ejes y cómo lo interpretas?**

Como en los pasos previos ya se han escalado los datos a usar en las opciones de la función `prcomp()` para realizar el análisis PCA la opción de scale y center será igual a FALSE.

```{r}
PCA_result <- prcomp(DC_escalado_filtrado,scale. = FALSE,center = FALSE)
datos_clinicos_pca <- data.frame(PC1 = PCA_result$x[, 1],
                                 PC2 = PCA_result$x[, 2],
                                 Trial.ID = datos_clinicos_copy$Trial.ID)

```

Para responder a la pregunta de **"¿se aprecia cierta separabilidad entre las pacientes, según sus valores posibles de respuesta a la cirugía tras el tratamiento adyuvante?"**, se debe agregar a la información realizada previamente de PCA la variable que almacena la respuesta a la cirugía tras el tratamiento adyuvante. Esta es la variable categórica resp.pCR debido a que, esta clasifica a las pacientes en función de si han respondido satisfactoriamente al tratamiento neoadyudante o no.

```{r}
datos_clinicos_pca$resp.pCR <- datos_clinicos_copy$resp.pCR[match(rownames(datos_clinicos_pca), rownames(datos_clinicos_copy))]
```

A continuación se crea un gráfico de dispersión de las dos primeras componentes principales coloreando los puntos en función de los valores de la variable resp.pCR para poder determinar si hay cierta separabilidad entre las pacientes.

```{r}
proporcion_varianza <- PCA_result$sdev^2 / sum(PCA_result$sdev^2)
pca_plot <- ggplot(datos_clinicos_pca, aes(x = PC1, y = PC2, color = factor(resp.pCR))) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  labs(x = paste("PC1 (", round(proporcion_varianza[1] * 100, 1), "%)", sep = ""),
       y = paste("PC2 (", round(proporcion_varianza[2] * 100, 1), "%)", sep = ""),
       title = "PCA de los pacientes según respuesta a la cirugía (resp.pCR)") +
  theme_minimal() +
  theme(legend.title = element_blank())

# Mostrar el gráfico
print(pca_plot)
```

De este gráfico se puede interpretar que la varianza explicada por el primer componente principal (PC1) es del 26,1%, lo que indica que este componente captura aproximadamente una cuarta parte de la variabilidad total en los datos.Mientras que,la varianza explicada por el segundo componente principal (PC2) es del 15,2%, lo que indica que este componente captura aproximadamente una séptima parte de la variabilidad total en los datos. Por tanto, al combinar la varianza explicada por PC1 y PC2, se obtiene un 41,3% de la varianza total. Esto significa que la proyección bidimensional de los datos en PC1 y PC2 captura aproximadamente el 41,3% de la información contenida en el conjunto de datos original. En el gráfico se pueden observar que los puntos rosas se corresponde con el valor 0 de la variables resp.pCR, es decir, se corresponde con los pacientes que no han respondido bien al tratamiento adyuvante previo a la cirugia, mientras que los puntos azules corresponden con el valor 1 de la variable, es decir, corresponde con los pacientes que si han respondido bien a dicho tratamiento.Basándonos en el gráfico, se puede decir que hay cierta separabilidad entre las pacientes, según sus valores posibles de respuesta a la cirugía tras el tratamiento adyuvante, ya que los puntos rosas (valor 0) se concentran en el segundo y tercer cuadrante y los puntos azules (valor 1) en el primer y cuarto cuadrante. Esto sugiere que hay una relación entre la respuesta a la cirugía y las variables representadas en los ejes del gráfico. Sin embargo, la separabilidad no es completa ya que hay puntos rosas en el primer y cuarto cuadrante y puntos azules en el segundo y tercer cuadrante.

Para responder a la pregunta **"¿Qué variables serían las más relevantes en ambos ejes y cómo lo interpretas?"**, se procede a examinar los vectores de cargas (loadings) del PCA. Los vectores de cargas indican cómo están relacionadas las variables originales con los componentes principales. Las cargas son coeficientes que indican cómo cada variable contribuye a la formación de cada componente principal.

```{r}
loadings <- PCA_result$rotation
```

Una vez obtenidos los vectores de cargas, se procede a examinar las cargas de cada variable en los dos primeros componentes principales.

```{r}
loadings[, 1:2]
```

En el eje x, que representa el componente principal con una varianza explicada del 26.1%, se observa que la variable con la mayor carga positiva es "Danaher.Treg" (0.285), seguida por "Danaher.NK.CD56dim.cells", "Danaher.B.cells", "Danaher.CD45", "Danaher.DC", "Danaher.Macrophages" y "CIN.Prop", todas ellas con cargas positivas moderadas. En cambio, "TIDE.TAM.M2" es la variable con la mayor carga negativa(-0.3), seguida por "TIDE.Exclusion" y "PGR.log2.tpm", con cargas negativas moderadas.

En el eje y, que representa el segundo componente principal con una varianza explicada del 15.2%, las variables más importantes con cargas positivas son "Danaher.Mast.cells" (0.326), "TIDE.Dysfunction" (0.25), "TIDE.CAF" (0.105), "Danaher.Neutrophils" (0.024) y "median_lymph_KDE_knn_50" (0.027) mientras que, las variables con las cargas negativas más importantes son "TIDE.MDSC" (-0.367) y "HRD.TelomericAI" (-0.331).

Por tanto, se deduce que estas cargas sugieren que las variables más importantes en la separación de las observaciones en el espacio de las dos primeras componentes principales son principalmente aquellas relacionadas con la respuesta inmunológica, como la presencia de diferentes tipos de células inmunes (por ejemplo, Danaher.B.cells, Danaher.Macrophages, Danaher.Neutrophils), los perfiles de expresión génica (por ejemplo, PGR.log2.tpm) y los índices de respuesta a la inmunoterapia (por ejemplo, TIDE.TAM.M2, TIDE.Dysfunction).

Por último,cabe añadir que esto solo se refiere a los dos primeros componentes principales, y puede haber otras variables que contribuyan significativamente a la variabilidad en componentes adicionales.

## Predicción de respuesta al tratamiento 

### Pregunta 7

**Elabora sendos modelos de regresión logística para predecir la respuesta al tratamiento, según la variable `resp.pCR` elaborando modelos por separado para cada uno de los cuatro grupos de datos: clínicos, de análisis digital, del sistema inmune y de transcriptómica. Analiza los resultados generados por el comando `summary(glm(modelo, family="binomial))` según las preguntas ¿es el modelo generado adecuado o se detectan problemas al generarlo? ¿cuál es su capacidad predictiva? ¿podemos determinar los predictores relevantes a la vista del resultado de la función `summary()`?**

La regresión logística es una técnica estadística utilizada para modelar la relación entre una variable dependiente binaria y una o más variables independientes. En el contexto de esta práctica, la predicción de la respuesta al tratamiento, la variable dependiente es la respuesta al tratamiento neoadyuvante (resp.pCR) y las variables independientes son las diferentes variables proporcionadas por los conjuntos de datos de datos clínicos, análisis digital, sistema inmune y transcriptómica.

El objetivo de la regresión logística es desarrollar un modelo que pueda predecir la probabilidad de respuesta al tratamiento en función de las variables independientes. El modelo resultante se puede utilizar para identificar las variables más importantes que influyen en la respuesta al tratamiento y para predecir la probabilidad de respuesta de un paciente en particular. Esto puede ser útil en la toma de decisiones clínicas y en el diseño de tratamientos personalizados para los pacientes.

#### Fichero Main.csv:

1.  Se comprueba que la varianza no este cerca o sea igual a cero ya que,se busca que las variables incluidas en un modelo tengan cierto grado de variabilidad para que sean útiles en la predicción. Si una variable tiene varianza cercana a cero, significa que no aporta información útil al modelo, debido a que no varía en las observaciones y no puede explicar ninguna variación en la variable de respuesta. Para hacer esto se usa función `nearZeroVar()` de la biblioteca caret en R que identifica automáticamente variables que tienen varianzas muy bajas o cercanas a cero.

```{r}
caret::nearZeroVar(DC_escalado_filtrado)
```

La función anterior nos devuelve un vector vacío de tipo entero, esto nos indica que no hay ninguna variable con una varianza cercana a 0.

2.  Se procede a unir el dataset que contiene las variables numéricas no altamente relacionadas con la variable respuesta resp.pCR.

```{r}
resp.pCR <- datos_clinicos_copy$resp.pCR
DC_escalado_final <- cbind(DC_escalado_filtrado,resp.pCR)
```

3.  Se procede a hacer una división los datos del dataset **DC_escalado_final** en subconjuntos de entrenamiento y prueba.

```{r}
set.seed(456)
trainIndex_DC <- createDataPartition(DC_escalado_final$resp.pCR, p = 0.80, list = FALSE)
DC_train <- DC_escalado_final[trainIndex_DC, ]
DC_test <- DC_escalado_final[-trainIndex_DC, ]
```

4.  Entrenamiento de un modelo de regresión logística para el conjunto de los datos del análisis clínico.

```{r}
modelo_DCGLM <- glm(resp.pCR ~ ., data = DC_train, family = "binomial")
summary(modelo_DCGLM)
```

Gracias a la implementación de la función `summary()` se obtiene la salida del modelo generado a través de la regresión logística. De esta salida se observa:

-   La fórmula utilizada para el modelo. En este caso, la variable respuesta es resp.pCR, y el modelo utiliza todas las variables restantes para predecir resp.pCR.

-   La sección **"Deviance Residuals"** muestra la distribución de los residuos del modelo. En este caso, la distribución de los residuos no es simétrica y hay algunos valores extremos, ya que el valor mínimo de los residuos es -2.0933 y el valor máximo es 2.3581.

-   La sección **"Coefficients"** muestra los coeficientes estimados para cada variable independiente. Los coeficientes indican la dirección y magnitud de la relación entre la variable independiente y la variable respuesta. Un coeficiente positivo indica que un aumento en la variable independiente se relaciona con un aumento en la probabilidad de resp.pCR. Un coeficiente negativo indica que un aumento en la variable independiente se relaciona con una disminución en la probabilidad de resp.pCR. Los coeficientes con un valor p \<0.05 se consideran significativos y se marcan con asteriscos. En este caso la variable Age.at.diagnosis y la ERBB2.log2.tpm son las que se consideran significativas en este modelo.Cabe añadir que la variable ER.Allred se encuentra marcada con un punto debido a que su valor, a pesar de ser superior a 0.05, esta muy cercana a esta valor.

-   La sección **"Null deviance"** muestra la desviación nula del modelo, que es la desviación del modelo nulo que solo tiene el intercepto.En este caso ese valor es 136.513.

-   La sección **"Residual deviance"** muestra la desviación residual del modelo, que es la desviación del modelo completo después de ajustar los coeficientes. La desviación residual se utiliza para evaluar qué tan bien se ajusta el modelo a los datos. En este caso, la desviación residual del modelo es de 75.937, lo que indica que el modelo se ajusta razonablemente bien a los datos.

-   La sección **"AIC"** proporciona el criterio de información de Akaike (AIC) del modelo. El AIC es una medida de la calidad del modelo que penaliza modelos complejos. En este caso, el valor del AIC es 133.94, lo que indica que el modelo tiene un buen equilibrio entre ajuste y complejidad.

-   La sección **"Number of Fisher Scoring iterations"** indica cuántas iteraciones fueron necesarias para estimar los coeficientes del modelo. En este caso, se realizaron 6 iteraciones.

El siguiente paso es calcular la matriz de confusión, esta es útil para evaluar la precisión del modelo y comprender la proporción de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos. Para calcular la matriz de confusión se usará el siguiente código que lo que hace es utilizar el modelo de regresión logística generado previamente para hacer predicciones de la variable de respuesta **resp.pCR** en el conjunto de datos de prueba **DC_test**. La función `predict()` devuelve las probabilidades de pertenencia a la clase positiva "1". Luego, se establece un umbral de corte de 0.5 para transformar estas probabilidades en una predicción binaria "1" o "0", utilizando la función `ifelse()`. Finalmente, se evalúa la precisión del modelo mediante la construcción de una matriz de confusión con la función `confusionMatrix()` , que compara las predicciones del modelo con las respuestas reales en el conjunto de datos de prueba, y proporciona varias métricas para evaluar la capacidad predictiva del modelo, incluyendo la precisión, la sensibilidad y la especificidad. El umbral de 0.5 se establece comúnmente porque es un valor intermedio que maximiza la precisión general del modelo en muchos casos.Además,este umbral es comúnmente utilizado en modelos de regresión logística binaria.

```{r}
pred_DCGLM <- predict(modelo_DCGLM, newdata = DC_test, type = "response")
pred_DCGLM2 <- ifelse(pred_DCGLM > 0.5, "1","0")
confusionMatrix(data = factor(pred_DCGLM2), reference = factor(DC_test$resp.pCR), positive = "1")
```

De la salida de este código se puede interpretrar:

-   La **matriz de confusión** muestra los resultados de la predicción del modelo. En este caso, la matriz es de 2x2, donde los valores reales se encuentran en las filas y las predicciones en las columnas. Los valores diagonales de la matriz corresponden a las predicciones correctas (verdaderos negativos y verdaderos positivos), mientras que los valores fuera de la diagonal corresponden a las predicciones incorrectas (falsos positivos y falsos negativos).En este caso, la matriz de confusión muestra que hay 17 verdaderos negativos (casos correctamente clasificados como no respondedores al tratamiento) y 4 verdaderos positivos (casos correctamente clasificados como respondedores al tratamiento). También hay 3 falsos negativos (casos clasificados incorrectamente como no respondedores cuando en realidad sí respondieron) y 4 falsos positivos (casos clasificados incorrectamente como respondedores cuando en realidad no respondieron).

-   La **exactitud o accuracy** del modelo es del 0,75, lo que significa que el 75% de las predicciones del modelo son correctas. \* El **intervalo de confianza (CI) al 95%** es de 0,5513-0,8931, lo que sugiere que la precisión del modelo podría variar dentro de este rango.

-   La **tasa de información nula (No Information Rate)** es también del 0,75, que es la tasa de exactitud que se podría lograr simplemente prediciendo siempre la clase mayoritaria (en este caso la clase 0).

-   El **valor de kappa** es de 0,3636, lo que indica que hay una correlación moderada entre las predicciones del modelo y las verdaderas clases.

-   El **parámetro P-Value [Acc \> NIR]** indica el valor p de la prueba de hipótesis que evalúa si la precisión (accuracy) del modelo es significativamente diferente de la tasa de no información (no information rate, NIR). En este caso, el valor p es de 0.5997, lo que significa que no hay suficiente evidencia para rechazar la hipótesis nula de que la precisión del modelo no es significativamente diferente del NIR. En otras palabras, no podemos concluir que el modelo sea significativamente mejor que simplemente predecir siempre la clase más frecuente en los datos.

-   La **prueba de Mcnemar** no es significativa (p-value \> 0.05), lo que sugiere que no hay una diferencia significativa entre las proporciones de errores cometidos por el modelo y por el método de referencia.

-   La **sensibilidad** del modelo es del 0,5714, lo que indica que el modelo puede identificar correctamente el 57,14% de los casos verdaderos positivos en la muestra de prueba.

La **especificidad** es del 0,8095, lo que significa que el modelo puede identificar correctamente el 80,95% de los casos verdaderos negativos.

-   El **valor predictivo positivo (Pos Pred Value)** es de 0,5, lo que significa que solo el 50% de las predicciones positivas del modelo son correctas.

-   El **valor predictivo negativo (Neg Pred Value)** es del 0,85, lo que indica que el modelo puede identificar correctamente el 85% de los casos verdaderos negativos.

-   La **prevalencia** es del 0,25, lo que significa que el 25% de las observaciones en la muestra de prueba tienen la clase positiva.

-   La **tasa de detección (Detection Rate)** es del 0,1429, lo que indica la proporción de casos verdaderos positivos que se identifican correctamente por el modelo.

-   La **prevalencia de detección (Detection Prevalence)** es del 0,2857, que es la proporción de casos positivos detectados por el modelo.

-   La **precisión equilibrada (Balanced Accuracy)** es del 0,6905, lo que indica la precisión promedio para ambas clases (0 y 1) del modelo.

También se puede calcular la curva ROC(Receiver Operating Characteristic).Esta es una herramienta gráfica utilizada en la evaluación de modelos de clasificación binaria. La curva representa la relación entre la tasa de verdaderos positivos (sensibilidad) y la tasa de falsos positivos (especificidad) a través de diferentes umbrales de clasificación.

La curva ROC es útil porque permite evaluar la capacidad de discriminación de un modelo de clasificación en diferentes umbrales de probabilidad. En otras palabras, la curva ROC ayuda a evaluar el equilibrio entre la tasa de verdaderos positivos (identificación correcta de los casos positivos) y la tasa de falsos positivos (clasificación incorrecta de los casos negativos como positivos) en diferentes puntos de corte.

La diagonal que va del punto (0,0) al (1,1) representa el rendimiento de un modelo de clasificación aleatorio, es decir, aquel que no tiene capacidad de discriminar entre las dos clases. Por encima de la diagonal, un modelo tiene una mayor sensibilidad que especificidad, y por debajo de la diagonal, un modelo tiene una mayor especificidad que sensibilidad. La curva ROC ideal se sitúa en el punto superior izquierdo del gráfico, lo que indica que el modelo tiene una sensibilidad del 100% y una especificidad del 100%.

La medida de área bajo la curva ROC (AUC) es una medida de la capacidad discriminativa global del modelo. El valor de AUC varía entre 0 y 1, siendo 0.5 equivalente al rendimiento de un modelo de clasificación aleatorio y 1 al rendimiento perfecto. Un AUC de 0.7 a 0.8 se considera moderado, mientras que un AUC de 0.8 o superior se considera bueno o excelente.

En general, la curva ROC es una herramienta valiosa para evaluar el rendimiento de un modelo de clasificación binaria y la medida AUC es una métrica útil para comparar modelos y evaluar su capacidad discriminativa global.

Para realizar la curva ROC primero, se definen las etiquetas verdaderas para el conjunto de prueba (DC_test\$resp.pCR) y se utiliza el modelo generado (pred_DCGLM) para predecir las probabilidades de la respuesta positiva para cada muestra en el conjunto de prueba.A continuación, el código utiliza la función `roc()` del paquete pROC para calcular la curva ROC a partir de las etiquetas verdaderas y las probabilidades de predicción. Se crea un dataframe curva_roc con los valores de especificidad y sensibilidad calculados a partir de la curva ROC.El valor de AUC se calcula a partir de la curva ROC utilizando la función `auc ()` también del paquete pROC.Finalmente, el código traza la curva ROC utilizando ggplot2, con el eje x representando la especificidad y el eje y representando la sensibilidad. Además, se agrega un área sombreada debajo de la curva ROC con el comando geom_ribbon para mostrar el área bajo la curva

```{r}
labels <- DC_test$resp.pCR
roc_obj <- roc(labels, pred_DCGLM)

curva_roc <- data.frame(
  specificity = 1 - roc_obj$specificities,
  sensitivity = roc_obj$sensitivities
)

auc <- round(auc(roc_obj), 3)
ggplot(data = curva_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_ribbon(aes(ymin = 0, ymax = sensitivity, xmin = 0, xmax = 1 - specificity),
              alpha = 0.2, fill = "red") +
  labs(x = "1-Especificidad", y = "Sensibilidad") +
  ggtitle(paste("Curva ROC del modelo DCGLM", "\nAUC =", auc))

```

EL valor de AUC de 0.816 sugiere que el modelo tiene una capacidad moderada para distinguir entre los casos positivos y negativos. Sin embargo, la interpretación final del valor de AUC dependerá del contexto específico de la aplicación y de la evaluación de otras métricas de rendimiento del modelo.

**¿Es el modelo generado adecuado o se detectan problemas al generarlo?¿Cuál es su capacidad predictiva?**

Según la información proporcionada, la evaluación del modelo muestra una capacidad predictiva moderada pero no excelente.Un valor de AUC de 0.816 sugiere que el modelo puede distinguir moderadamente bien entre los casos positivos y negativos, mientras que una precisión o accuracy del 0.75 , lo que significa que el 75% de las predicciones del modelo son correctas, y la tasa de información nula es del 0,75, lo que sugiere que el modelo no está realizando predicciones peores que simplemente predecir siempre la clase mayoritaria. Sin embargo, el valor de kappa es de solo 0,3636, lo que indica que la correlación entre las predicciones del modelo y las verdaderas clases es moderada, sugiriendo que el modelo puede no ser muy confiable en algunas situaciones. Además, el valor predictivo positivo (Pos Pred Value) es de solo 0,5, lo que significa que solo el 50% de las predicciones positivas del modelo son correctas, lo que indica que el modelo puede estar subestimando la probabilidad de que un paciente responda al tratamiento.

En general, aunque el modelo parece tener una capacidad razonable para predecir la respuesta al tratamiento, su rendimiento puede ser mejorado.

#### Fichero DigPathology.tsv

Para realizar un modelo de regresión logística para predecir la respuesta al tratamiento, según la variable resp.pCR en el conjunto de datos análisis digital se procede a repetir los pasos realizados anteriormente.

1.  Comprobación de la que varianza no sea cercana o igual a cero con la función `nearZeroVar()`.

```{r}
caret::nearZeroVar(AD_escalado)
```

La función anterior nos devuelve un vector vacío de tipo entero, esto nos indica que no hay ninguna variable con una varianza cercana o igual a 0.

2.  Se procede a unir el dataset de datos clínicos escalado con el dataset de análisis digital escalado por la columna Trial.ID, con la función `merge()`para poder unir la variable respuesta resp.pCR al dataset análisis digital y poder evaluar este dataset en función de dicha variable.

```{r}
DCAD <- merge(DC_escalado,AD_escalado, by = "Trial.ID")
#Se eliminan las columnas del dataset DC_escalado
DCAD <- DCAD[,-c(1:55)]
DCAD <- DCAD[,-c(2:18)]
```

3.  Una vez que te quedas con las columnas correspondientes al conjunto de datos análisis digital y la variable respuesta resp.pCR, se procede a la división de datos.

```{r}
set.seed(456)
trainIndex_AD <- createDataPartition(DCAD$resp.pCR, p = 0.80, list = FALSE)
AD_train <- DCAD[trainIndex_AD, ]
AD_test <- DCAD[-trainIndex_AD, ]
```

4.  Entrenamiento de un modelo de regresión logística para el conjunto de los datos del análisis digital.

```{r}
modelo_ADGLM <- glm(resp.pCR ~ ., data = AD_train, family = "binomial")
summary(modelo_ADGLM)
```

La salida proporcionada por la función `summary()` del modelo lineal generalizado es la siguiente:

-   La sección **"Deviance Residuals"** muestra la distribución de los residuos del modelo. En este caso, los valores mínimos y máximos son -1.3597 y 2.5084, respectivamente. La distribución de los residuos no es simétrica, lo que indica que el modelo podría no estar ajustando adecuadamente a algunos datos.

-   La sección **"Coefficients"** muestra los coeficientes estimados para cada variable independiente.En este caso, solo la variable "median_cancer_KDE_knn_50" tiene un valor p \<0.05 y se considera significativa en este modelo.

-   La sección **"Null deviance"** muestra la desviación nula del modelo, que es la desviación del modelo nulo que solo tiene el intercepto. En este caso, ese valor es 136.51.

-   La sección **"Residual deviance"** muestra la desviación residual del modelo, que es la desviación del modelo completo después de ajustar los coeficientes. En este caso, la desviación residual del modelo es de 115.62, lo que indica que el modelo se ajusta razonablemente bien a los datos.

-   La sección **"AIC"** proporciona el criterio de información de Akaike (AIC) del modelo. En este caso, el valor del AIC es 131.62, lo que indica que el modelo tiene un buen equilibrio entre ajuste y complejidad.

-   La sección **"Number of Fisher Scoring iterations"** indica cuántas iteraciones fueron necesarias para estimar los coeficientes del modelo. En este caso, se realizaron 6 iteraciones.

5.  Para evaluar la capacidad predictiva del modelo:

```{r}
pred_ADGLM <- predict(modelo_ADGLM, newdata = AD_test, type = "response")
pred_ADGLM2 <- ifelse(pred_ADGLM > 0.5, "1","0")
confusionMatrix(data = factor(pred_ADGLM2), reference = factor(AD_test$resp.pCR), positive = "1")
```

De la salida de este código se puede interpretrar:

-   La **matriz de confusión** muestra que el modelo ha clasificado correctamente 18 observaciones (17 verdaderos negativos y 1 verdadero positivo), mientras que ha clasificado incorrectamente 10 observaciones (6 falsos negativos y 4 falsos positivos).

-   La **exactitud o accuracy** del modelo es del 0.6429, lo que significa que el 64.29% de las predicciones del modelo son correctas.

-   El **intervalo de confianza al 95%** es de 0.4407-0.8136, lo que sugiere que la precisión del modelo podría variar dentro de este rango.

-   La **tasa de información nula (No Information Rate)** es del 0.75, que es la tasa de exactitud que se podría lograr simplemente prediciendo siempre la clase mayoritaria (en este caso la clase 0).

-   El **parámetro P-Value [Acc \> NIR]** indica la probabilidad de que la precisión del modelo sea igual o mayor que la tasa de no información (No Information Rate, NIR) por casualidad. El NIR es la precisión que se obtendría si se predijera siempre la clase más común en los datos de prueba, es decir, si se asumiera que la respuesta es siempre la misma. En este caso, el valor de 0.9321 indica que la precisión del modelo no es significativamente mejor que el NIR, lo que sugiere que el modelo no es muy útil para predecir la respuesta al tratamiento en el conjunto de datos de análisis digital de DigPathology.

-   El **valor de kappa es de -0.0526**, lo que indica una correlación baja o incluso nula entre las predicciones del modelo y las verdaderas clases.

-   La **prueba de Mcnemar** no es significativa (p-value \> 0.05), lo que sugiere que no hay una diferencia significativa entre las proporciones de errores cometidos por el modelo y por el método de referencia.

-   La **sensibilidad** del modelo es del 0.14286, lo que indica que el modelo puede identificar correctamente solo el 14.29% de los casos verdaderos positivos en la muestra de prueba.

-   La **especificidad** es del 0.80952, lo que significa que el modelo puede identificar correctamente el 80.95% de los casos verdaderos negativos.

-   El **valor predictivo positivo (Pos Pred Value)** es de 0.2, lo que significa que solo el 20% de las predicciones positivas del modelo son correctas.

-   El **valor predictivo negativo (Neg Pred Value)** es del 0.73913, lo que indica que el modelo puede identificar correctamente el 73.91% de los casos verdaderos negativos.

-   La **prevalencia** es del 0.25, lo que significa que el 25% de las observaciones en la muestra de prueba tienen la clase positiva.

-   La **tasa de detección (Detection Rate)** es del 0.03571, lo que indica la proporción de casos verdaderos positivos que se identifican correctamente por el modelo.

-   La **prevalencia de detección (Detection Prevalence)** es del 0.17857, que es la proporción de casos positivos detectados por el modelo.

-   La **precisión equilibrada (Balanced Accuracy)** es del 0.47619, lo que indica la precisión promedio para ambas clases (0 y 1) del modelo.

Para realizar la curva ROC, se usa el mismo código que para el conjunto de datos denominado datos clínicos pero usando en este caso el conjunto de datos de análisis digital.

```{r}
labels <- AD_test$resp.pCR
roc_obj <- roc(labels, pred_ADGLM)
curva_roc <- data.frame(
  specificity = 1 - roc_obj$specificities,
  sensitivity = roc_obj$sensitivities
)

auc <- round(auc(roc_obj), 3)
ggplot(data = curva_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_ribbon(aes(ymin = 0, ymax = sensitivity, xmin = 0, xmax = 1 - specificity),
              alpha = 0.2, fill = "red") +
  labs(x = "1-Especificidad", y = "Sensibilidad") +
  ggtitle(paste("Curva ROC del modelo ADGLM", "\nAUC =", auc))

```

Un valor de AUC de 0.735 sugiere que el modelo tiene una capacidad moderada para distinguir entre los casos positivos y negativos. Es decir, el modelo es capaz de clasificar correctamente a los pacientes con cierta precisión, pero no con una precisión excelente. Sin embargo, la interpretación final del valor de AUC dependerá del contexto específico de la aplicación y de la evaluación de otras métricas de rendimiento del modelo.

**¿Es el modelo generado adecuado o se detectan problemas al generarlo?¿Cuál es su capacidad predictiva?**

Según los resultados anteriores, teniendo en cuenta un valor de AUC de 0.735, se puede decir que el modelo tiene una capacidad moderada para distinguir entre los casos positivos y negativos.Sin embargo,el modelo generado no parece ser adecuado debido a que la mayoría de las variables en el modelo no son significativas, lo que sugiere que no están relacionadas de manera significativa con la respuesta al tratamiento. Además,la baja capacidad predictiva del modelo es un indicador importante de que el modelo no es adecuado para predecir la respuesta al tratamiento. La precisión del modelo, que es del 64,29%, es relativamente baja, lo que significa que el modelo solo puede predecir con precisión el 64,29% de los casos. La sensibilidad del modelo, que es del 14,29%, también es baja, lo que significa que el modelo no puede identificar correctamente la mayoría de los casos verdaderos positivos. En general, esta evaluación muestra que el modelo no tiene una buena capacidad predictiva en el conjunto de datos de análisis digital, ya que la exactitud, la sensibilidad y el valor predictivo positivo son muy bajos. Además, el valor de kappa indica que el modelo no está mejorando significativamente la predicción en comparación con el método de referencia.Por ello, sería adecuado considerar otros modelos para mejorar la capacidad predictiva del modelo.

#### Fichero mutational-signatures.tsv.

Para realizar un modelo de regresión logística para predecir la respuesta al tratamiento, según la variable resp.pCR en el conjunto de datos análisis digital se procede a repetir los pasos realizados anteriormente.

1.  Comprobación de la que varianza no sea cercana o igual a cero con la función `nearZeroVar()`.

```{r}
caret::nearZeroVar(SI_escalado)
```

Esta salida muestra una lista con los índices de las columnas de SI_escalado que se consideran cercanas o iguales a cero en cuanto a su varianza. Es decir, estas columnas tienen muy poca variabilidad y, por lo tanto, se procede a quitarlas para entrenar el modelo ya que, estas tienen muy poca variabilidad y, por lo tanto, son menos informativas para el modelo.

2.  Eliminación de las columnas con varianza cercana a 0.

```{r}
var_eliminar <- nearZeroVar(SI_escalado)
SI_escalado_filtrado <- SI_escalado[,-var_eliminar]
```

3.Se procede a unir el dataset de datos clínicos escalado con el dataset de sistema inmune escalado por la columna Trial.ID, con la función `merge()`para poder unir la variable respuesta resp.pCR al dataset SI_escalado_filtrado y poder evaluar este dataset en función de dicha variable.

```{r}
colnames(SI_escalado_filtrado)[colnames(SI_escalado_filtrado) == "X"] <- "Trial.ID"
DCSI <- merge(DC_escalado, SI_escalado_filtrado, by = "Trial.ID")
#Se eliminan las columnas del dataset DC_escalado
DCSI <- DCSI[,-c(1:55)]
DCSI <- DCSI[,-c(2:18)]
```

Con el código anterior, se cambia el nombre de la columna que contiene los identificadores de los pacientes del dataset SI_escalado_filtrado para poder unirlo con el dataset DC_escalado. Posteriormente se eliminas las columnas que no nos interesan para realizar el modelo.

4.  Una vez que te quedas con las columnas correspondientes al conjunto de datos sistema inmune y la variable respuesta resp.pCR, se procede a la división de datos.

```{r}
set.seed(456)
trainIndex_SI <- createDataPartition(DCSI$resp.pCR, p = 0.80, list = FALSE)
SI_train <- DCSI[trainIndex_SI, ]
SI_test <- DCSI[-trainIndex_SI, ]
```

5.Entrenamiento de un modelo de regresión logística para el conjunto de los datos del sistema inmune.

```{r}
modelo_SIGLM <- glm(resp.pCR ~ ., data = SI_train, family = "binomial")
summary(modelo_SIGLM)
```

La salida proporcionada por la función `summary()` del modelo lineal generalizado es la siguiente:

-   La sección **"Coefficients"** muestra los coeficientes estimados para cada variable independiente.En este caso, solo la variable Signature.15 tiene un valor p \<0.05 y se considera significativa en este modelo.Cabe destacar que Signature.1 y Signature.24 estan marcadas con un punto debido a que su p-value esta muy cercano al valor de 0.05, pudiendose considerar significativas.

-   La **desviación residual**.La tabla indica que los valores mínimos, el primer cuartil, la mediana, el tercer cuartil y el máximo de las desviaciones residuales son -1.6823, -0.7010, -0.6196, 0.8312 y 1.8495, respectivamente.

-   La **desviación nula y la desviación residual** se proporcionan en la salida, con la desviación residual menor que la desviación nula, lo que sugiere que el modelo ajustado es mejor que el modelo nulo, que no tiene predictores.

-   El **AIC (criterio de información de Akaike)** en este caso, el valor de AIC es 137.2.

-   El **número de iteraciones de Fisher Scoring** indica que se ha iterado 5 veces el algoritmo de maximización de la verosimilitud para ajustar el modelo a los datos.

6.  Para evaluar la capacidad predictiva del modelo:

```{r}
pred_SIGLM <- predict(modelo_SIGLM, newdata = SI_test, type = "response")
pred_SIGLM2 <- ifelse(pred_SIGLM > 0.5, "1","0")
confusionMatrix(data = factor(pred_SIGLM2), reference = factor(SI_test$resp.pCR), positive = "1")
```

De la salida de este código se puede interpretrar:

-   **La matriz de confusión** muestra que el modelo clasificó correctamente 19 observaciones y clasificó erróneamente 9 observaciones. En este caso, el modelo predijo correctamente 17 casos que pertenecen a la clase 0 y 2 casos que pertenecen a la clase 1.Mientras que, el modelo predijo incorrectamente 5 casos que pertenecen a la clase 1 y 4 casos que pertenecen a la clase 0.

-   El **intervalo de confianza del 95%** para la precisión del modelo. En este caso, se estima que la precisión del modelo en el conjunto de datos de prueba se encuentra entre 0.4765 y 0.8412.

-   **No Information Rate** muestra que la clase más común es "no pCR" y representa el 75% de los casos.

-   **P-Value [Acc \> NIR]** el valor p es 0.8615, lo que indica que no hay evidencia estadística para rechazar la hipótesis nula de que la precisión del modelo no es mejor que la del "No Information Rate".

-   La **precisión o "Accuracy"**. En este caso, la precisión del modelo es del 67,86%, lo que indica que el modelo clasifica correctamente aproximadamente el 68% de las observaciones.

-   La **sensibilidad y la especificidad** se muestran como "Sensitivity" y "Specificity", respectivamente. En este caso, la sensibilidad del modelo es del 28,57%, lo que indica que el modelo identifica correctamente solo el 28,57% de las respuestas completas o parciales. La especificidad es del 80,95%, lo que indica que el modelo identifica correctamente el 80,95% de las respuestas incompletas.

-   El **valor de kappa** es de 0,1, lo que indica una concordancia mínima entre las predicciones del modelo y las respuestas reales.

Para realizar la curva ROC, se usa el mismo código que para el conjunto de datos denominado datos clínicos pero usando en este caso el conjunto de datos de sistema inmune.

```{r}
labels <- SI_test$resp.pCR
roc_obj <- roc(labels, pred_SIGLM)
curva_roc <- data.frame(
  specificity = 1 - roc_obj$specificities,
  sensitivity = roc_obj$sensitivities
)

auc <- round(auc(roc_obj), 3)
ggplot(data = curva_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_ribbon(aes(ymin = 0, ymax = sensitivity, xmin = 0, xmax = 1 - specificity),
              alpha = 0.2, fill = "red") +
  labs(x = "1-Especificidad", y = "Sensibilidad") +
  ggtitle(paste("Curva ROC del modelo SIGLM", "\nAUC =", auc))
```

Un valor de curva de 0.66 indica que el modelo tiene una capacidad moderada para distinguir entre los casos positivos y negativos, pero no es muy preciso en sus predicciones. Es decir, el modelo tiene cierta capacidad predictiva, pero hay margen de mejora. Es importante considerar otros factores, como la precisión y la sensibilidad del modelo, para obtener una evaluación completa de su capacidad predictiva.

**¿Es el modelo generado adecuado o se detectan problemas al generarlo?¿Cuál es su capacidad predictiva?**

Con un valor AUC de 0.66, se puede concluir que el modelo tiene una capacidad moderada para distinguir entre los casos positivos y negativos. Sin embargo, la evaluación exhaustiva del modelo indica que su capacidad predictiva es limitada, con baja sensibilidad y mínima concordancia entre las predicciones del modelo y las respuestas reales. Por lo tanto, se puede sugerir que se requiere una evaluación más exhaustiva del modelo, como la validación cruzada o la comparación con otros modelos para mejorar su capacidad predictiva.

#### Fichero RNAseq-rawcounts.tsv.

Para realizar un modelo de regresión logística para predecir la respuesta al tratamiento, según la variable resp.pCR elaborando del conjunto en el conjunto de datos transcriptomica se procede a ejecutar los siguientes pasos.

1.  Seleccion de la columna "resp.pCR" del dataset "datos_clinicos_copy" utilizando la función `select()` de dplyr y almacenarla en una nueva variable llamada resp.pCR.

```{r}
resp.pCR <- select(datos_clinicos_copy, resp.pCR)
```

2.  Unir el dataset escalado de datos clínicos "DC_escalado" con el dataset de trascriptómica "transcriptomica_t" utilizando la función `merge()`, uniendo las filas por la columna "Trial.ID". El resultado se almacena en un nuevo dataset llamado **DCtrans**.

```{r}
DCtrans<-merge(DC_escalado, transcriptomica_t, by = "Trial.ID")
```

3.Almacenar los identificadores de los pacientes del dataset DCtrans en una nueva variable llamada **Trial.ID_transDC**. Posteriormente,se procede a eliminar las primeras 55 columnas y las columnas 2 a 18 de DCtran ya que estas contienen información del dataset DC_escalado.

```{r}
Trial.ID_transDC<-DCtrans$Trial.ID
DCtrans <- DCtrans[,-c(1:55)]
DCtrans <- DCtrans[,-c(2:18)]
```

4.  Convertir las variables numéricas de DCtrans en variables numéricas y filtrar aquellas variables que tengan varianza cercana a cero mediante la función nearZeroVar().

```{r}
trans_var_num <- apply(DCtrans, 2, as.numeric)
nzv_indices <- nearZeroVar(trans_var_num)
trans_var_filtrado <- trans_var_num[, -nzv_indices]
```

5.  Escalar las variables filtradas de DCtrans utilizando la función `scale()` para que tengan media cero y desviación estándar unitaria.

```{r}
trans_var_filtrado_escalado <- scale(trans_var_filtrado)
```

6.  Calcular la matriz de correlación de Pearson entre las variables de DCtrans filtradas y escaladas, y encontrar las variables altamente correlacionadas utilizando la función `findCorrelation()`.

```{r,eval=FALSE}
cor_trans <- cor(trans_var_filtrado_escalado)
variables_descartar_t <- findCorrelation(cor_trans, cutoff = 0.8)
saveRDS(variables_descartar_t, "variables_descartar_t")
```

7.  Eliminar las variables altamente correlacionadas del dataset de trascriptómica.

```{r}
variables_descartar_t<-readRDS("/home/alumno25/ML/variables_descartar_t")
trans_var_filtrado_escalado1 <- trans_var_filtrado_escalado[,-variables_descartar_t]
```

8.  Realizar un Análisis de Componentes Principales (PCA) sobre el dataset de trascriptómica utilizando la función prcomp() de R y seleccionar las k primeras componentes principales que explican al menos el 90% de la varianza total.Esto se hace para reducir el número de variables a analizar en el modelo y evitar el error de stack overflow que se produce debido a que tiene demasiadas variables en el modelo. El PCA nos permite transformar las variables originales en un conjunto de variables no correlacionadas llamadas componentes principales, que son una combinación lineal de las variables originales. Al seleccionar las k primeras componentes principales que explican al menos el 90% de la varianza total, se mantienen la mayor cantidad posible de información en un número reducido de variables, lo que permite ajustar un modelo más simple y más interpretable.

```{r}
pca <- prcomp(trans_var_filtrado_escalado1)
pca_varianza <- pca$sdev^2/sum(pca$sdev^2)
sum_pca_varianza <- cumsum(pca_varianza)
k <- which(sum_pca_varianza >= 0.9)[1]
```

9.  Se crean nuevas variables con las k primeras componentes principales seleccionadas en el análisis de PCA utilizando la función `predict()` y se guarda en la variable "trans_pca".A continuación, se unen las variables creadas en el paso anterior con la variable respuesta "resp.pCR" utilizando la función `cbind()`, y se guarda en la variable "trans_final".

```{r}
trans_pca <- predict(pca, newdata = trans_var_filtrado_escalado)[, 1:k]
trans_final <- cbind(trans_pca, resp.pCR)
```

10. Se procede a dividir el conjunto de datos en entrenamiento y prueba utilizando la función `createDataPartition()`, en este caso, el 80% de los datos se utiliza para entrenar el modelo y el 20% para probarlo. El índice de división se guarda en la variable "trainIndex_trans".A continuación,se crea el conjunto de datos de entrenamiento utilizando la variable "trainIndex_trans" seleccionando las filas correspondientes a los índices de entrenamiento y se guarda en la variable "trans_train". Del mismo modo, se crea el conjunto de datos de prueba seleccionando las filas correspondientes a los índices que no están en el conjunto de entrenamiento y se guarda en la variable "trans_test".

```{r}
set.seed(456)
trainIndex_trans <- createDataPartition(trans_final$resp.pCR, p = 0.8, list = FALSE)
trans_train <- trans_final[trainIndex_trans,]
trans_test <- trans_final[-trainIndex_trans,]
```

11.Se entrena un modelo de regresión logística utilizando la función `glm()` con la variable respuesta "resp.pCR" y las variables predictoras en el conjunto de datos de entrenamiento "trans_train", y se guarda en la variable "modelo_transGLM". Finalmente, se guarda el modelo en un archivo RDS con la función saveRDS().

```{r,eval=FALSE}
modelo_transGLM <- glm(resp.pCR ~ ., data = trans_train, family = "binomial")
saveRDS(modelo_transGLM,"modelo_transGLM")
modelo_transGLM<-readRDS("/home/alumno25/ML/modelo_transGLM")
```

```{r}
modelo_transGLM<-readRDS("/home/alumno25/ML/modelo_transGLM")
modelo_transGLM
```

\*La advertencia "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred" indica que la probabilidad estimada de la respuesta es muy cercana a 0 o 1, lo que sugiere que el modelo puede estar sobreajustando los datos y que no es capaz de generalizar correctamente.

La salida proporcionada por la función `summary()` del modelo lineal generalizado es la siguiente:

-   La salida del modelo ajustado indica que se han ajustado coeficientes para cada una de las 111 componentes principales seleccionadas por el análisis de componentes principales. El coeficiente para cada componente principal indica el cambio en la log-odds de la respuesta por cada unidad de cambio en la componente principal. El coeficiente del intercepto se puede interpretar como el log-odds de la respuesta cuando todas las componentes principales son iguales a cero.

-   El valor AIC es de 224, lo que sugiere que el modelo es relativamente complejo y puede estar sobreajustando los datos. Además, la tabla muestra que el modelo tiene 7 grados de libertad residual, lo que sugiere que el modelo se ajusta a un número limitado de datos de prueba.

12. Para evaluar la capacidad predictiva del modelo:

```{r}
pred_transGLM <- predict(modelo_transGLM,trans_test, type = "response")
pred_transGLM2 <- ifelse(pred_transGLM > 0.5, "1","0")
confusionMatrix(data = factor(pred_transGLM2), reference = factor(trans_test$resp.pCR), positive = "1")
```

Esta salida corresponde a la matriz de confusión y las estadísticas de evaluación del modelo predictivo.

-   La **matriz de confusión** nos aporta la información de que el modelo hizo 21 predicciones correctas de la clase 0 y 6 predicciones correctas de la clase 1. También hubo un falso positivo, es decir, una predicción de la clase 1 donde la verdadera clase era 0.

-   La **precisión (Accuracy)** del 0.9643, lo que significa que el modelo predijo correctamente el 96.43% de los casos.

-   **El intervalo de confianza del 95%** para la exactitud está entre el 81,65% y el 99,91%.

-   La **tasa de información nula (No Information Rate)** es del 75%, lo que significa que el modelo supera significativamente la tasa de predicción aleatoria.

-   El **valor de P-Value [Acc \> NIR]** (valor de p para la prueba de hipótesis de que la precisión del modelo es mayor que la tasa de información nula) fue menor que 0.05, lo que sugiere que la precisión del modelo es significativamente mayor que la tasa de información nula.

-   La **sensibilidad (Sensitivity)** del modelo fue del 0.8571, lo que indica que el modelo identificó correctamente el 85.71% de los casos positivos (verdaderos positivos).

-   La **especificidad (Specificity)** del modelo fue del 1.0000, lo que significa que el modelo identificó correctamente el 100% de los casos negativos (verdaderos negativos).

-   La **prueba de McNemar** arroja un valor de p de 1, lo que indica que no hay una diferencia significativa entre los modelos comparados.

-   El **valor de Kappa**, que es una medida de la concordancia entre las predicciones del modelo y los valores reales, fue de 0.9, lo que indica una buena concordancia.

Para realizar la curva ROC, se usa el mismo código que para el conjunto de datos denominado datos clínicos pero usando en este caso el conjunto de datos de transcriptómica.

```{r}
labels <- trans_test$resp.pCR
roc_obj <- roc(labels, pred_transGLM)
curva_roc <- data.frame(
  specificity = 1 - roc_obj$specificities,
  sensitivity = roc_obj$sensitivities
)

auc <- round(auc(roc_obj), 3)
ggplot(data = curva_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_ribbon(aes(ymin = 0, ymax = sensitivity, xmin = 0, xmax = 1 - specificity),
              alpha = 0.2, fill = "red") +
  labs(x = "1-Especificidad", y = "Sensibilidad") +
  ggtitle(paste("Curva ROC del modelo transGLM", "\nAUC =", auc))


```

Un valor de AUC de 0.864 sugiere que el modelo tiene una capacidad moderada a alta para distinguir entre los casos positivos y negativos. Sin embargo, el valor de AUC por sí solo no es suficiente para evaluar completamente la capacidad predictiva del modelo. Sería necesario realizar una evaluación exhaustiva del modelo.

**¿Es el modelo generado adecuado o se detectan problemas al generarlo?**

En la salida del modelo GLM generado a partir de los datos de transcriptómica se observa un warning que indica que el algoritmo no ha convergido adecuadamente, lo que puede sugerir que el modelo no es del todo adecuado. Además, se detecta que algunas de las probabilidades ajustadas son extremadamente cercanas a 0 o 1, lo que puede ser un indicio de problemas en el ajuste del modelo.

**¿Cuál es su capacidad predictiva?**

Con un valor de AUC de 0.864, se puede concluir que el modelo generado tiene una capacidad alta para distinguir entre los casos positivos y negativos.Además, para evaluar la capacidad predictiva del modelo, se ha generado una matriz de confusión que muestra la cantidad de predicciones correctas e incorrectas. La matriz de confusión indica que el modelo ha predicho correctamente 27 de los 28 casos en el conjunto de prueba, lo que sugiere que tiene una buena capacidad predictiva.Además, la métrica de precisión positiva (Pos Pred Value) del modelo es de 1, lo que indica que cuando el modelo predice que un paciente tiene una respuesta completa patológica (pCR), lo hace correctamente en todos los casos. La sensibilidad del modelo es de 0.86, lo que sugiere que el modelo es capaz de detectar correctamente la mayoría de los casos con respuesta completa patológica.En general, el modelo tiene una alta precisión (0.9643) y sensibilidad (0.8571) para predecir la variable objetivo, y una especificidad del 100%. La estadística Kappa también indica un alto nivel de concordancia entre el modelo y los valores reales. Cabe destacar que, el hecho de que el modelo GLM no haya convergido adecuadamente y que algunas de las probabilidades ajustadas sean extremadamente cercanas a 0 o 1 puede afectar la matriz de confusión debido a que, cuando el modelo no converge adecuadamente, significa que los coeficientes estimados no son los óptimos y, por lo tanto, los resultados del modelo pueden ser menos precisos o incluso incorrectos. Esto puede llevar a una matriz de confusión que no refleje correctamente la capacidad del modelo para predecir los valores verdaderos y falsos.

**¿Podemos determinar los predictores relevantes a la vista del resultado de la función summary()?**

De manera general, los predictores relevantes se pueden identificar a partir del resultado de la función `summary()`. Sin embargo, es necesario realizar un análisis más detallado utilizando técnicas de selección de características o análisis de importancia de características para determinar si hay otros predictores que puedan ser relevantes . Además, aunque se han identificado algunas variables significativas en los distintos modelo, es importante tener en cuenta que la inclusión de estas variables no garantiza una buena capacidad predictiva del modelo y que también se deben considerar otras variables que puedan mejorar la precisión del modelo. Por lo tanto, se requiere una evaluación más exhaustiva de los datos para determinar los predictores más relevantes para la variable objetivo.

### Pregunta 8

**Haciendo uso de Caret, elabora sendos modelos de bosques aleatorios haciendo uso de un paquete como `ranger` o similar. Crea modelos por separado para cada uno de los cuatro grupos de datos: clínicos, de análisis digital, del sistema inmune y de transcriptómica. Analiza los resultados desde el punto de vista de la relevancia de los predictores correspondientes usando `varImp()`, representando los correspondientes plots facilitados por Caret para ello. Comenta los resultados. Nota: añade `importance="permutation"` a la llamada `train()` correspondiente si haces uso de `ranger` para que `varImp()` funcione correctamente.**

Random Forest es un método versátil de aprendizaje automático capaz de realizar tanto tareas de regresión como de clasificación. También lleva a cabo métodos de reducción dimensional, trata valores perdidos, valores atípicos y otros pasos esenciales de exploración de datos. Es un tipo de método de aprendizaje por conjuntos, donde un grupo de modelos débiles se combinan para formar un modelo poderoso.

En Random Forest se ejecutan varios algoritmos de árbol de decisiones en lugar de uno solo. Para clasificar un nuevo objeto basado en atributos, cada árbol de decisión da una clasificación y finalmente la decisión con mayor "votos" es la predicción del algoritmo.

Para realizar esta práctica se va a usar el algortimo ranger. Este es una implementación rápida de bosques aleatorios o partición recursiva, particularmente adecuada para datos de alta dimensión.

Para visualizar los hiperparámetros del algoritmo ranger se realiza el siguiente código:

```{r}
modelLookup(model="ranger")
```

En esta salida se pueden observar los hiperparámetros de dicho algoritmo:

-   **mtry**: Número de variables predictoras que se deben considerar en cada división de árbol.

-   **splitrule**: La regla de división de nodos que se debe utilizar. Puede ser "gini" para el índice de Gini o "extratrees" para la división extra aleatoria.

-   **min.node.size**: El número mínimo de observaciones que se deben tener en un nodo para poder ser dividido.

Para empezar el análisis de los distintos modelos empleando la técnica de random forest, se crea un objeto denomidado control_RF usando la función `trainControl()` que especifica la configuración para la validación cruzada. En este caso, se utiliza la validación cruzada con 10 particiones. Se especifica seeds = NULL, lo que significa que se utilizará la semilla global establecida con set.seed para cada iteración de validación cruzada.Con la opción returnResamp = "final" se especifica que solo se debe devolver el modelo ajustado final, en lugar de devolver todos los modelos ajustados en cada iteración de validación cruzada. Con la opción search = "random" se especifica que se debe realizar una búsqueda aleatoria de hiperparámetros en lugar de una búsqueda exhaustiva de todos los posibles valores de hiperparámetros. La búsqueda aleatoria de hiperparámetros permite que el modelo explore una amplia gama de valores de hiperparámetros de manera aleatoria, lo que puede llevar a encontrar combinaciones óptimas de hiperparámetros que no se hubieran considerado de otra manera. Esto es especialmente útil debido a que no se conocen los valores óptimos de hiperparámetros y se desea encontrar el mejor modelo posible. Por último, la opción verboseIter = FALSE especifica que no se deben imprimir los detalles de cada iteración de validación cruzada.

```{r}
set.seed(456)
control_RF <- trainControl(method = "cv",
                        number = 10,
                        seeds = NULL,
                        returnResamp = "final",
                        search = "random",
                        verboseIter = FALSE)

```

#### Fichero Main.csv

Dado que en la sección anterior ya se realizó la división de los datos en conjuntos de entrenamiento y prueba para el conjunto de datos clínicos, no es necesario volver a hacerlo.

1.  Se utiliza la función `train()` de la librería caret para entrenar un modelo de clasificación utilizando el algoritmo de bosques aleatorios con la biblioteca ranger. La variable objetivo a predecir es resp.pCR, y las características utilizadas para hacer la predicción se especifican con el argumento \~.. Los datos utilizados para entrenar el modelo es la partición de training de los datos clínicos denominado DC_train. El preprocesamiento de los datos se desactiva con preProcess = NULL ya que, los datos ya han sido previamente preprocesados. El objeto control_RF se utiliza para especificar la configuración de la validación cruzada y la búsqueda aleatoria de hiperparámetros. El modelo se ajusta utilizando 10 iteraciones de validación cruzada y una longitud de búsqueda de hiperparámetros de 30. Por último,la importancia de las características se calcula utilizando la técnica de permutación.

```{r,eval=FALSE}
modelo_RFDC <- caret::train(resp.pCR ~ ., data = DC_train,
                      preProcess = NULL,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength = 25,
                      importance = "permutation")
saveRDS(modelo_RFDC,"modelo_RFDC")
```

2.  Para observar el resultado del entrenamiento del modelo de clasificación utilizando bosques aleatorios:

```{r}
modelo_RFDC<-readRDS("/home/alumno25/ML/modelo_RFDC")
modelo_RFDC
```

Esta salida corresponde a los resultados de un modelo de Random Forest entrenado con un conjunto de datos que tiene 119 muestras y 28 características (predictores) para predecir una variable objetivo que tiene dos clases, '0' y '1'. El modelo se ha evaluado mediante la validación cruzada con 10 fold y se han probado distintas combinaciones de parámetros para ajustar el modelo, como el tamaño mínimo del nodo (min.node.size), el número de características a considerar en cada árbol (mtry) y la regla de división del árbol (splitrule).

En este caso, el modelo con los parámetros óptimos tuvo una precisión de 0.765, lo que significa que el 76,5% de las observaciones se clasificaron correctamente y un valor del coeficiente kappa de 0.318, lo que indica una concordancia moderada entre las clasificaciones del modelo y las clasificaciones esperadas.Además, los hiperparámetros óptimos del modelo fueron mtry = 11, splitrule = gini y min.node.size = 7. Esto significa que el modelo usó 18 predictores aleatorios en cada árbol de decisión, utilizó la medida de impureza de Gini para dividir los nodos del árbol y permitió que los nodos terminales tuvieran al menos 7 observaciones.

3.  Para determinar los predictores más importantes en el modelo se utiliza `varImp()`.

```{r}
importancia_DCRF <- varImp(modelo_RFDC)
print(importancia_DCRF)
```

4.  Para visualizar las variables más importantes se usa el siguiente código:

```{r}
plot(importancia_DCRF, col = "blue", main = "Importancia de las variables de datos clínicos", pch = 16, lty = 2)
```

De las dos salidas anteriores se puede interpretar que la salida del modelo de bosques aleatorios proporciona una medida de la importancia relativa de cada variable en la predicción de la variable objetivo, que en este caso es la respuesta al tratamiento adyuvante.En este caso, se puede observar que las variables que más contribuyen a la capacidad predictiva del modelo:

-   **median_lymph_KDE_knn_50:** esta variable tiene la importancia más alta, lo que indica que es la variable más importante para la predicción de la respuesta pCR.

-   **PGR.log2.tpm:** esta variable tiene una importancia relativamente alta en la predicción de la respuesta pCR.

Las variables restantes tienen importancia relativamente baja en la predicción de la respuesta pCR.

5.  Para evaluar la capacidad predictiva del bosque aleatorio en el conjunto de datos de datos clínicos, se realiza la matriz de confusión:

```{r}
predicciones_RFDC <- predict(modelo_RFDC, newdata = DC_test)
confusionMatrix(data = predicciones_RFDC, reference = DC_test$resp.pCR,positive = "1")
```

De esta salida se puede interpretar que el modelo tiene una precisión global (accuracy) del 82%, lo que indica que es capaz de predecir correctamente la clase en un alto porcentaje de casos. Además, el valor de especificidad es del 100%, lo que indica que el modelo no clasifica como positivos a casos negativos. Sin embargo, la sensibilidad es muy baja (29%), lo que significa que el modelo tiene dificultades para detectar los verdaderos positivos. El valor kappa es 0.375, lo que indica una concordancia moderada entre las predicciones del modelo y las observaciones reales. Esto sugiere que el modelo tiene cierta capacidad predictiva, pero no es muy preciso en la predicción de la clase positiva. Por tanto, aunque el modelo tiene una precisión global alta, su baja sensibilidad indica que puede haber un alto número de falsos negativos, lo que puede tener implicaciones clínicas graves. Por lo tanto, sería necesario realizar mejoras en el modelo para garantizar su eficacia en la toma de decisiones clínicas.

#### Fichero DigPathology.tsv

Dado que en la sección anterior ya se realizó la división de los datos en conjuntos de entrenamiento y prueba para el conjunto de datos de análisis digital, no es necesario volver a hacerlo. Los pasos para el análisis de datos digitales serán los mismos que para los datos clínicos, excepto por el conjunto de datos utilizado para entrenar y predecir el modelo, que en este caso serán los conjuntos de entrenamiento y prueba específicos de los datos de análisis digital.

1.  Entrenamiento del modelo.

```{r,eval=FALSE}
modelo_RFAD <- caret::train(resp.pCR ~ ., data = AD_train,
                      preProcess = NULL,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength = 25,
                      importance = "permutation")
saveRDS(modelo_RFAD,"modelo_RFAD")
```

2.  Para observar el resultado del entrenamiento del modelo de clasificación utilizando bosques aleatorios:

```{r}
modelo_RFAD<-readRDS("/home/alumno25/ML/modelo_RFAD")
modelo_RFAD
```

Esta salida corresponde a los resultados de un modelo de Random Forest entrenado con un conjunto de datos que tiene 119 muestras y 7 características (predictores) para predecir una variable objetivo que tiene dos clases, '0' y '1'. El modelo se ha evaluado mediante la validación cruzada con 10 fold y se han probado distintas combinaciones de parámetros para ajustar el modelo, como el tamaño mínimo del nodo (min.node.size), el número de características a considerar en cada árbol (mtry) y la regla de división del árbol (splitrule).

En la tabla de resultados se muestra el valor de la métrica de Accuracy y el valor de Kappa para cada combinación de parámetros. En este caso,el modelo seleccionado como óptimo es el que obtiene el mayor valor de Accuracy, y se ha obtenido con los siguientes valores de parámetros: mtry = 7, splitrule = gini y min.node.size = 19. Esto indica que el modelo tiene una precisión global del 76.48%, lo que indica que es capaz de predecir correctamente la clase en un porcentaje razonable de casos. El valor de kappa de 0.2781 indica una concordancia moderada entre las predicciones del modelo y las observaciones reales, teniendo en cuenta la probabilidad aleatoria de acertar por azar. Esto sugiere que el modelo tiene cierta capacidad predictiva, pero no es muy preciso en la predicción de la clase positiva.

3.  Evaluación de la importancia de las variables predictoras en el modelo de bosques aleatorios entrenado (modelo_RFAD) utilizando la función `varImp()` de la librería Caret.

```{r}
importancia_RFAD <- varImp(modelo_RFAD)
print(importancia_RFAD)
```

4.  Para visualizar las variables más importantes se usa el siguiente código:

```{r}
plot(importancia_RFAD, col = "orange", main = "Importancia de las variables de análisis digital usando el modelo de bosque aleatorio", pch = 16, lty = 2)
```

De las dos salidas anteriores se obtiene la información de que la variable con el valor de importancia más alto es "median_lymph_KDE_knn_50", con un valor de 100.0. Esto significa que esta variable es la más importante para la predicción del modelo. Las variables "median_cancer_KDE_knn_50" y "fraction_cancer" también tienen valores de importancia relativamente altos, con 83.61 y 55.57, respectivamente. Las variables "fraction_stromal", "fraction_lymph", "n_total" y "median_stromal_KDE_knn_50" tienen valores de importancia más bajos, teniendo la última un valor de 0. Por tanto,se puede decir que las variables relacionadas con las características de los ganglios linfáticos y la fracción de tejido canceroso parecen ser las más importantes para la predicción del modelo.

5.  Para evaluar la capacidad predictiva del bosque aleatorio en el conjunto de datos de análisis digital, se realiza la matriz de confusión:

```{r}
predicciones_RFAD <- predict(modelo_RFAD, newdata = AD_test)
confusionMatrix(data = predicciones_RFAD, reference = AD_test$resp.pCR,positive = "1")
```

De esta salida se puede interpretar que la precisión global (accuracy) del modelo es del 75%, lo que indica que es capaz de predecir correctamente la clase en un porcentaje razonable de casos. El valor de kappa es de 0.125, lo que indica una concordancia mínima entre las predicciones del modelo y las observaciones reales, teniendo en cuenta la probabilidad aleatoria de acertar por azar. El valor de sensibilidad es bajo (14.3%), lo que indica que el modelo tiene dificultades para detectar los verdaderos positivos. Por otro lado, la especificidad es alta (95.2%), lo que significa que el modelo no clasifica como positivos a casos negativos. En general, aunque el modelo tiene una precisión global razonable, su baja sensibilidad y baja concordancia con las observaciones reales indican que es necesario mejorar su rendimiento antes de utilizarlo para tomar decisiones clínicas.

#### Fichero mutational-signatures.tsv

Dado que en la sección anterior ya se realizó la división de los datos en conjuntos de entrenamiento y prueba para el conjunto de datos del sistema inmune, no es necesario volver a hacerlo. Los pasos para el análisis de datos digitales serán los mismos que para el conjunto de datos clínicos, excepto por el conjunto de datos utilizado para entrenar y predecir el modelo, que en este caso serán los conjuntos de entrenamiento y prueba específicos de los datos de sistema inmune.

1.  Entrenamiento del modelo.

```{r,eval=FALSE}
modelo_RFSI <- caret::train(resp.pCR ~ ., data = SI_train,
                      preProcess = NULL,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength = 25,
                      importance = "permutation")
saveRDS(modelo_RFSI,"modelo_RFSI")
```

2.  Para observar el resultado del entrenamiento del modelo de clasificación utilizando bosques aleatorios:

```{r}
modelo_RFSI<-readRDS("/home/alumno25/ML/modelo_RFSI")
modelo_RFSI
```

Esta salida muestra los resultados de un modelo de Random Forest aplicado a un conjunto de datos con 115 muestras y 10 variables predictoras. El proceso de validación cruzada con 10 pliegues se utilizó para ajustar los parámetros del modelo y evaluar su rendimiento. Se probaron varias combinaciones de los parámetros min.node.size, mtry y splitrule y se evaluó la precisión del modelo en función de cada combinación.

En este caso,la combinación óptima de los parámetros se determinó según el criterio de exactitud (accuracy) y los valores óptimos seleccionados son mtry = 3, splitrule = gini y min.node.size = 16.El valor de accuracy seleccionado como óptimo es de 0.68, esto quiere decir que el modelo tiene una precisión global del 68.79%, lo que indica que es capaz de predecir correctamente la clase en un porcentaje razonable de casos.Sin embargo, el valor de Kappa de 0.0344 indica una concordancia muy baja entre las predicciones del modelo y las observaciones reales, teniendo en cuenta la probabilidad aleatoria de acertar por azar. Esto sugiere que el modelo tiene muy poca capacidad predictiva y que probablemente sea necesario mejorar el algoritmo de aprendizaje utilizado.

3.  Evaluación de la importancia de las variables predictoras en el modelo de bosques aleatorios entrenado (modelo_RFSI) utilizando la función `varImp()` de la librería Caret.

```{r}
importancia_RFSI <- varImp(modelo_RFSI)
print(importancia_RFSI)
```

4.  Para visualizar las variables más importantes se usa el siguiente código:

```{r}
plot(importancia_RFSI, col = "green", main = "Importancia de las variables de sistema inmuune usando el modelo de bosque aleatorio", pch = 16, lty = 2)
```

De las dos salidas anteriores se puede observar la importancia de cada variable en el modelo, ordenadas por su importancia decreciente. La columna "NumberMutationsAnalysed" tiene una importancia del 100%, lo que indica que es la variable más importante en la clasificación de las muestras. Las otras variables, como "Signature.24", "Signature.3", "Signature.1", "Signature.13", "Unknown", "Signature.15", "Signature.2", "Signature.10" y "Signature.7", tienen una importancia menor en comparación con "NumberMutationsAnalysed".Por tanto, se concluye que la variable "NumberMutationsAnalysed" es la más importante para el modelo.

5.  Para evaluar la capacidad predictiva del bosque aleatorio en el conjunto de datos de sistema inmune, se realiza la matriz de confusión:

```{r}
predicciones_RFSI <- predict(modelo_RFSI, newdata = SI_test)
confusionMatrix(data = predicciones_RFSI, reference = SI_test$resp.pCR,positive = "1")
```

De esta salida se puede interpretar que el parámetro de la exactitud indica que el modelo tiene una tasa de éxito del 64.29% al clasificar correctamente los casos. La tasa de verdaderos positivos (sensibilidad) es muy baja, con solo el 14.29% de los casos correctamente clasificados como verdaderos positivos, mientras que la tasa de verdaderos negativos (especificidad) es relativamente alta, con el 80.95% de los casos correctamente clasificados como verdaderos negativos.Además,el valor de kappa de -0.0526 indica que el modelo no está mejorando significativamente la predicción en comparación con una clasificación aleatoria, lo que sugiere que el modelo no tiene una capacidad predictiva significativa en este conjunto de datos.

#### Fichero RNAseq-rawcounts.tsv

Dado que en la sección anterior ya se realizó la división de los datos en conjuntos de entrenamiento y prueba para el conjunto de datos de transcriptómica, no es necesario volver a hacerlo. Los pasos para el análisis de datos de transcriptómica serán los mismos que para el conjunto de datos clínicos, excepto por el conjunto de datos utilizado para entrenar y predecir el modelo, que en este caso serán los conjuntos de entrenamiento y prueba específicos de los datos de transcriptómica.

1.  Entrenamiento del modelo.

```{r,eval=FALSE}
modelo_RFtrans <- caret::train(resp.pCR ~ ., data = trans_train,
                      preProcess = NULL,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength = 25,
                      importance = "permutation")
saveRDS(modelo_RFtrans,"modelo_RFtrans")
```

2.  Para observar el resultado del entrenamiento del modelo de clasificación utilizando bosques aleatorios:

```{r}
modelo_RFtrans<-readRDS("/home/alumno25/ML/modelo_RFtrans")
modelo_RFtrans
```

De esta salida se puede interpretar que en este modelo de Random Forest, se están utilizando 111 predictores para clasificar entre dos clases (0 y 1) con 119 muestras en el conjunto de datos. Se aplicó la técnica de validación cruzada con 10 folds para evaluar el desempeño del modelo en diferentes combinaciones de parámetros de ajuste (min.node.size, mtry, splitrule). Sin embargo, se observa que la precisión del modelo (Accuracy) se mantiene constante en 0.7393939 en todas las combinaciones de parámetros probadas, lo que indica que el modelo no está aprendiendo información relevante del conjunto de datos. El valor de Kappa en todas las combinaciones de parámetros es 0, lo que indica que el modelo no está clasificando mejor que un clasificador aleatorio. Por lo tanto, este modelo no es útil para hacer predicciones precisas en el conjunto de datos.

3.  Evaluación de la importancia de las variables predictoras en el modelo de bosques aleatorios entrenado (modelo_RFtrans) utilizando la función `varImp()` de la librería Caret.

```{r}
importancia_RFtrans <- varImp(modelo_RFtrans)
print(importancia_RFtrans)
```

En esta salida se observa la importancia de cada componente principal (PC) en el modelo de bosques aleatorios. En este caso la componente principal PC2 tiene la importancia relativa más alta, con un valor de 100. Las otras componentes principales tienen importancias relativas menores, que van desde 36.13804 para PC48 hasta 16.55135 para PC49. Esto sugiere que el modelo de bosques aleatorios puede estar utilizando principalmente la información contenida en la segunda componente principal para hacer predicciones.

Con esta salida y la anterior se puede interpretar que como el modelo introducido es un análisis de PCA, es posible que la variabilidad de los datos esté capturada principalmente por unas determinadas componenets principales, lo que hace que los predictores restantes sean redundantes o irrelevantes.Por ello, se van a seleccionar las tres primeras componentes princiaples más relevantes (PC2,PC48 y PC19) para entrenar un segundo modelo de bosques aleatorios.

4.  Segundo entrenamiendo del modelo.

```{r,eval=FALSE}
modelo_RFtrans2 <- caret::train(resp.pCR ~ PC2+PC48+PC19, data = trans_train,
                      preProcess = NULL,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength = 25,
                      importance = "permutation")
saveRDS(modelo_RFtrans2,"modelo_RFtrans2")
```

5.  Para observar el resultado del entrenamiento del segundo modelo de clasificación utilizando bosques aleatorios:

```{r}
modelo_RFtrans2<-readRDS("/home/alumno25/ML/modelo_RFtrans2")
modelo_RFtrans2
```

En esta salida se puede observar que el modelo final seleccionado utilizó mtry = 3, splitrule = gini y min.node.size = 12, lo que resultó en una precisión del modelo de 0.789 y un valor de kappa de 0.406. Estos resultados sugieren que el modelo es relativamente preciso y que tiene una buena capacidad para clasificar nuevas muestras. Al comparar las dos salidas de los modelos generados con los datos de transcriptómica se obseva que cuando solo se usaron las tres componentes principales más importantes como predictores, la precisión del modelo mejoró a alrededor del 78%, y el Kappa aumentó significativamente a alrededor del 0.36. Esto sugiere que el modelo se desempeñó mejor en la clasificación de las muestras cuando se usaron solo las tres componentes principales más importantes en lugar de todas las componentes principales.Por lo tanto, se puede decir que usar solo las tres componentes principales más importantes como predictores es una mejor opción para construir un modelo de Random Forest en este caso.

6.  Para identificar aquellos genes que tienen una mayor influencia en las componenetes principales más importantes, se ejecuta el siguiente código:

```{r}
pca_loadings <- pca$rotation
genes_PC2 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC2"]),decreasing = TRUE)[1:20], ])
genes_PC2                                   
```

```{r}
genes_PC48 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC48"]),decreasing = TRUE)[1:20], ])
genes_PC48
```

```{r}
genes_PC19 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC19"]),decreasing = TRUE)[1:20], ])
genes_PC19
```

7.  Para evaluar la capacidad predictiva del bosque aleatorio en el conjunto de datos de transcriptómica, se realiza la matriz de confusión:

```{r}
predicciones_RFtrans <- predict(modelo_RFtrans2, newdata = trans_test)
confusionMatrix(data = predicciones_RFtrans, reference = trans_test$resp.pCR,positive = "1")
```

De esta salida se interpreta que la precisión del modelo es del 78,57%, lo que significa que el modelo predice correctamente el 78,57% de las observaciones. La sensibilidad del modelo es del 57,14%, lo que significa que el modelo identifica correctamente el 57,14% de las observaciones positivas. La especificidad del modelo es del 85,71%, lo que significa que el modelo identifica correctamente el 85,71% de las observaciones negativas.En general este modelo parece tener una precisión razonable, con una tasa de aciertos del 78.57%. Sin embargo, la sensibilidad del modelo es relativamente baja (57.14%), lo que significa que hay una probabilidad razonablemente alta de falsos negativos, es decir, de que el modelo clasifique erróneamente un caso positivo como negativo.Por último, el valor Kappa del modelo es moderado (0.4286), lo que indica una concordancia moderada entre las predicciones del modelo y las clases reales. Por tanto,el modelo parece ser una buena aproximación auqnue podría mejorarse en términos de su capacidad para detectar los casos positivos.

### Pregunta 9

**Haciendo uso de Caret, optimiza una red neuronal de la familia (mlpKeras) con los datos de Main.csv y compáralo cuantitativa y cualitativamente, con el modelo ranger análogo de la pregunta anterior. ¿Es posible obtener, con los medios que conocemos, una estimación sobre qué predictores considera la red neuronal como más importantes?**

En los últimos años, el Deep Learning ha experimentado un crecimiento exponencial en el ámbito de la inteligencia artificial, convirtiéndose en una herramienta esencial para abordar y resolver problemas complejos en diversos campos entre ellos la medicina. Las redes neuronales profundas, también conocidas como deep neural networks (DNNs), son el pilar fundamental de estas técnicas, permitiendo modelar relaciones no lineales de alto nivel en grandes conjuntos de datos.

Sin embargo, uno de los principales desafíos en el entrenamiento de redes neuronales profundas es evitar el sobreajuste, es decir, cuando el modelo se ajusta demasiado a los datos de entrenamiento y pierde capacidad de generalización en datos no vistos previamente. En este contexto, una estrategia ampliamente utilizada para abordar este problema es la técnica de Dropout, que consiste en desactivar aleatoriamente algunas neuronas durante el proceso de entrenamiento, mejorando así la robustez y la capacidad de generalización del modelo.

En este apartado, se explorará el uso de la librería MLPKerasDropout, una implementación de redes neuronales multicapa (MLP) basada en Keras, la cual integra la técnica de Dropout de manera eficiente y fácil de usar.

Para obtener información sobre los parámetros del modelo mlpKerasDropout, que es una implementación de una red neuronal multicapa con dropout en Keras.Se realiza el siguiente código:

```{r}
mdropout = getModelInfo("mlpKerasDropout")
mdropout$mlpKerasDropout$parameters
```

Los parámetros en el modelo MLPKerasDropout son:

-   **size:** se refiere al número de neuronas en la capa oculta de la red neuronal.

-   **dropout:** es la fracción de las unidades de la capa oculta que se establecen en cero durante el entrenamiento. Esto se hace para evitar el overfitting y mejorar la generalización del modelo.

-   **batch_size:** es el número de muestras que se utilizan para actualizar los pesos en cada iteración de entrenamiento.

-   **lr:** es la tasa de aprendizaje, que determina la magnitud de los cambios de peso en cada iteración de entrenamiento.

-   **rho:** es un coeficiente de regularización que penaliza los pesos grandes. Se utiliza para evitar el overfitting y mejorar la generalización del modelo.

-   **decay:** es la tasa de decaimiento de la tasa de aprendizaje a lo largo del tiempo. Se utiliza para reducir gradualmente la tasa de aprendizaje a medida que el modelo se acerca a la convergencia.

-   **activation:** es la función de activación utilizada en las capas ocultas de la red neuronal. Hay varias opciones comunes, como relu, sigmoid y tanh, y cada una tiene sus propias ventajas y desventajas dependiendo del conjunto de datos y la tarea de modelado.

En este apartado se van a evaluar paso a paso los parámetros de manera progresiva con el algortimo mlpKerasDropout en el conjunto de datos de entrenamiento que previamente se ha definido del conjunto de datos de análisis clínicos con el fin de encontrar la mejor estructura de la red neuronal (número de capas, funciones de activación para las capas y complejidad en número de nodos por capa) y la mejor combinación de valores para los hiperparámetros.

Para empezar se prepara el conjunto de datos para que pueda ser utilizado para entrenar un modelo de aprendizaje automático en el que se va a predecir la respuesta al tratamiento.Los niveles de la variable DC_trainy "neg" y "pos", corresponden a la respuesta 0 y 1 respectivamente.

```{r}
DC_trainx=DC_train
DC_trainy=DC_train$resp.pCR
DC_trainx$resp.pCR=NULL
levels(DC_trainy)<-c("neg","pos")
```

1.  Número de nodos.

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 1:10,
                        dropout =0,
                        batch_size = 5,
                        lr = 1e-4,
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=10)
saveRDS(model_RNDC, "1model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/1model_RNDC")
model_RNDC
```

De esta salida se interpreta:

La red neuronal tiene 119 muestras y 28 variables predictoras para clasificar en dos clases: 'neg' y 'pos'.El modelo no ha recibido ningún preprocesamiento debido a que los datos ya han sido preprocesados. Se ha aplicado la técnica de re-muestreo "Bootstrapped" con 10 repeticiones. Este modelo se ha optimizado utilizando el parámetro de tamaño de la red neuronal, variando su valor de 1 a 10.\
Se evaluó el rendimiento del modelo en términos de precisión (Accuracy) y kappa para cada valor de tamaño. La precisión máxima obtenida fue del 65,68% con un tamaño de red neuronal de 10, aunque este resultado puede ser sensible a la forma en que se ha realizado el re-muestreo.También se muestra que los parámetros de Dropout, batch_size, decay y activation se han mantenido constantes en los valores de 0, 0, 0 y sigmoid, respectivamente, mientras se optimizaba el tamaño de la red neuronal.El modelo final utilizado se ajustó con un tamaño de 10, Dropout = 0, batch_size = 5, lr = 1e-04, rho = 0, decay = 0 y activation = sigmoid.\
En general, el modelo no parece tener una gran precisión, por lo que se prosigue a ajustar otros parámetros para mejorar el rendimiento.

2.  Funciones de activación.

Las funciones de activación son una parte importante de las redes neuronales artificiales y se utilizan para introducir no linealidad en la salida de las neuronas.Las funciones de activación sigmoid, tanh y relu son tres de las funciones de activación más comunes utilizadas en las redes neuronales artificiales. La función sigmoid se utiliza para problemas de clasificación binaria y produce una salida entre 0 y 1. La función tanh también se utiliza para problemas de clasificación binaria y produce una salida entre -1 y 1. La función relu se utiliza para problemas de clasificación multiclase y produce una salida entre 0 y el valor de entrada.

Como anteriormente no se ha podido establecer un número de nodos, se procede a elegir 7 nodos que es un número conservador para poder seguir evaluando otros parámetros y poder ver si finalmente el hecho de modificar otros parámetros nos ayuda a indentificar el número de nodos óptimos para este modelo.

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =0,
                        batch_size = 5,
                        lr = 1e-4,
                        rho = 0,
                        decay = 0,
                        activation = c("sigmoid","relu","tanh"))

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=10)
saveRDS(model_RNDC, "2model_RNDC")
```

Visualización de los resultados del modelo

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/2model_RNDC")
model_RNDC
```

De esta salida se interpreta:

En el conjunto de datos que consta de 119 muestras y 28 variables predictoras,se muestran los resultados del re-muestreo "Bootstrapped" con 10 repeticiones para los parámetros de activación (sigmoid, relu y tanh), junto con su precisión y coeficiente kappa correspondientes.\
La función de activación tanh logró la mayor precisión del 58,75%, seguida de la función de activación relu con una precisión del 55,58%, mientras que la función sigmoid tuvo la menor precisión del 52,46%.El modelo se optimizó fijando los parámetros de tamaño y dropout en 7 y 0, respectivamente, y manteniendo constantes otros parámetros como batch_size, lr, rho y decay en 5, 1e-04, 0 y 0, respectivamente. La precisión se utilizó como métrica para seleccionar el modelo óptimo, y los parámetros finales utilizados para el modelo incluyen un tamaño de 7, un valor de dropout de 0, un tamaño de lote (batch_size) de 5, un lr de 1e-04, un rho de 0 y un decay de 0, y una función de activación de tanh.\
En general, el modelo no parece tener una precisión muy alta, por lo que se prosigue optimización otros parámetros.

3.  Aumento el número de epochs

El número de epochs es el número de veces que el algoritmo de aprendizaje recorre todo el conjunto de datos de entrenamiento.

Se recomienda aumentar el número de epochs debido a que esto puede ayudar a mejorar el rendimiento del modelo de Red Neuronal Perceptrón Multicapa (MLP) ya que, a medida que el modelo se entrena durante más epochs, tiene la oportunidad de ajustar mejor sus pesos y mejorar su capacidad para clasificar con precisión las observaciones del conjunto de datos de entrenamiento. Sin embargo, es importante encontrar un equilibrio entre el número de epochs y el riesgo de sobreajuste porque el aumento del número de epochs también puede llevar al sobreajuste (overfitting) del modelo, lo que conllevaría que el modelo se ajuste demasiado bien al conjunto de datos de entrenamiento y no se generalice bien los nuevos datos.

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =0,
                        batch_size = 5,
                        lr = 1e-4,
                        rho = 0,
                        decay = 0,
                        activation = c("sigmoid","relu","tanh"))

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=20)
saveRDS(model_RNDC, "3model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/3model_RNDC")
model_RNDC
```

De esta salida se interpreta:

El modelo se entrenó y evaluó utilizando un conjunto de datos de 119 muestras y 28 predictores para predecir dos clases: 'neg' y 'pos'.En la sección "Resampling results across tuning parameters", se muestran los resultados de la validación cruzada con bootstrap para diferentes valores de la función de activación: sigmoid, relu y tanh. La métrica utilizada para seleccionar el modelo óptimo fue la precisión (Accuracy).En este caso, el modelo con la función de activación sigmoid obtuvo la mayor precisión de 0.6424 y un coeficiente Kappa de 0.0893, lo que indica que el modelo tiene una precisión moderada para predecir las clases 'neg' y 'pos'. La función de activación tanh también obtuvo una precisión razonable de 0.6162 y un coeficiente Kappa de 0.1762. Por otro lado, la función de activación relu obtuvo una precisión menor de 0.5584 y un coeficiente Kappa de 0.0976.En la sección "Tuning parameter", se indica que el tamaño de la red (size) se mantuvo constante en 7, y no se aplicó Dropout en la red neuronal (dropout = 0). Además, se mantuvieron constantes otros parámetros como batch_size, lr, rho y decay.Por tanto,el aumento de epochs ha conllevado a que la función de activación sigmoid sea la mejor opción para este modelo de Red Neuronal Perceptrón Multicapa con Dropout con una configuración específica de parámetros para el modelo.

4.  Valor de dropout.

El valor de dropout es una técnica de regularización que se utiliza en las redes neuronales artificiales para reducir el sobreajuste. El sobreajuste ocurre cuando la red neuronal se ajusta demasiado a los datos de entrenamiento y no generaliza bien a los datos nuevos.El valor de dropout es la fracción de las neuronas que se desactivan aleatoriamente durante el entrenamiento. Esto ayuda a prevenir el sobreajuste y mejora la capacidad de generalización de la red neuronal.

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =c(0.1, 0.25, 0.5),
                        batch_size = 5,
                        lr = 1e-4,
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=20)
saveRDS(model_RNDC, "4model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/4model_RNDC")
model_RNDC
```

De esta salida se interpreta:

El modelo se entrenó y evaluó utilizando un conjunto de datos de 119 muestras y 28 predictores para predecir dos clases: 'neg' y 'pos'.En la sección "Resampling results across tuning parameters", se muestran los resultados de la validación cruzada con bootstrap para diferentes valores del parámetro Dropout: 0.1, 0.25 y 0.5. La métrica utilizada para seleccionar el modelo óptimo fue la precisión (Accuracy).En este caso, el modelo con un valor de Dropout de 0.1 obtuvo la mayor precisión de 0.6337 y un coeficiente Kappa de 0.0915, lo que indica que el modelo tiene una precisión moderada para predecir las clases 'neg' y 'pos'. Los modelos con valores más altos de Dropout (0.25 y 0.5) obtuvieron una precisión menor.En la sección "Tuning parameter", se indica que el tamaño de la red (size) se mantuvo constante en 7, y se mantuvieron constantes otros parámetros como batch_size, lr, rho y decay. Además, la función de activación utilizada fue sigmoid.Por tanto, esta salida indica que un valor de Dropout de 0.1 es la mejor opción para este modelo de Red Neuronal Perceptrón Multicapa con Dropout con una configuración específica de parámetros para el modelo.

5.  Learning rate.

El parámetro learning rate es un hiperparámetro que se utiliza para ajustar la velocidad de aprendizaje del modelo.

Para verificar si el valor establecido al parámetro learning rate es alto o bajo, cambiaremos sus valores.

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =0.1,
                        batch_size = 5,
                        lr = c(1e-3,1e-4,1e-5),
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=20)
saveRDS(model_RNDC, "5model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/5model_RNDC")
model_RNDC
```

De esta salida se interpreta:

Esta salida muestra los resultados del ajuste de un modelo de red neuronal artificial utilizando el algoritmo de MLP (perceptrón multicapa) con dropout y diferentes valores de tasa de aprendizaje (lr).La evaluación se realizó mediante validación cruzada bootstrapped con 10 repeticiones, y se utilizó la métrica de precisión (accuracy) y kappa para seleccionar el mejor modelo.Se mantuvieron constantes los valores de size=7, dropout=0 y decay=0, y se encontró que el mejor valor para la tasa de aprendizaje fue 0.001, obteniéndose una precisión de 0.7126 y un kappa de 0.1838. Esto sugiere que el modelo es capaz de predecir con mayor precisión la clase de los datos al reducir el valor de learning rate. Para verificar esto hay que aumentar el número de epochs ya que,al disminuir la tasa de aprendizaje (learning rate), se reduce la magnitud de los ajustes de los pesos de la red neuronal en cada iteración del entrenamiento, lo que significa que la red neuronal converge más lentamente hacia una solución óptima.Por lo tanto, si se disminuye la tasa de aprendizaje, se puede necesitar un mayor número de épocas de entrenamiento para alcanzar una precisión aceptable en el modelo. Aumentar el número de épocas permite que la red neuronal tenga más iteraciones de ajuste de pesos, lo que puede ayudar a que la red converja hacia una solución óptima.

6.  Aumento de las epochs a 40:

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =0.1,
                        batch_size = 5,
                        lr = c(1e-3,1e-4,1e-5),
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=40)
saveRDS(model_RNDC, "6model_RNDC")
```

Visualización de los resultados del modelo

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/6model_RNDC")
model_RNDC
```

De esta salida se puede interpretar que al aumentar el número de epochs la precisión del modelo ha mejorado en comparación con la salida anterior ya que se consigue una mayor precisión en la clasificación, con un valor de accuracy de 0.7302216 en comparación con el valor de 0.7126140 del modelo con 20 epochs. Además, el valor de kappa también es mayor en el modelo con 40 epochs, lo que indica una mayor consistencia en las predicciones del modelo. Esto sugiere que al aumentar el número de epochs se ha mejorado la capacidad del modelo para clasificar correctamente los datos de prueba.Esto sugiere que el modelo necesitaba más tiempo para aprender y ajustarse a los datos, y que aumentar el número de epochs ha permitido que el modelo alcance su verdadero potencial de precisión.

7.  Aumento del valor learning rate.

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =0.1,
                        batch_size = 5,
                        lr = c(1e-2,0.5*1e-2,1e-3),
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=20)
saveRDS(model_RNDC, "7model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/7model_RNDC")
model_RNDC
```

De esta salida se observa que el mejor valor para la tasa de aprendizaje fue 0.005, obteniendo una precisión de 0.7391 y un kappa de 0.3349. Esto sugiere que el modelo es capaz de predecir con mayor precisión la clase de los datos al aumentar el valor de lr. El tamaño de la red neuronal (size), la tasa de abandono (dropout), rho y decay se mantienen constantes en los valores previamente especificados.

El resultado final del modelo, selecciona la combinación de hiperparámetros con un tamaño de red de 7, tasa de abandono de 0.1, tamaño de lote (batch size) de 5, tasa de aprendizaje de 0.005, rho de 0, decay de 0 y función de activación sigmoidal.

8.Aumento de epochs

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 7,
                        dropout =0.1,
                        batch_size = 5,
                        lr = c(1e-2,0.5*1e-2,1e-3),
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=40)
saveRDS(model_RNDC, "8model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/8model_RNDC")
model_RNDC
```

De esta salida se puede interpretar que en comparación entre los dos modelos(7 y 8), el segundo modelo con 40 epochs es ligeramente mejor que el primer modelo con 20 epochs en términos de precisión y kappa. Además, ambos modelos tienen el mismo valor de learning rate (0.005), por lo que en este caso no es necesario aumentar el número de epochs para evaluar diferentes valores de learning rate. En general, es importante buscar un equilibrio entre el número de epochs y el learning rate para lograr el mejor desempeño del modelo.Por tanto, nos quedamos con e modelo 7 de 20 epochs y 0.005 de learning rate.

9.Número de nodos:

```{r,eval=FALSE}
set.seed(456)
trnCtrl = trainControl(method="boot",
                       number=10,
                       search = "grid",
                       classProbs = T)

mygrid = expand.grid(size = 1:10,
                        dropout =0.1,
                        batch_size = 5,
                        lr = 0.5*1e-2,
                        rho = 0,
                        decay = 0,
                        activation = "sigmoid")

model_RNDC= caret::train(x=as.matrix(DC_trainx),y=DC_trainy,
                     preProcess=NULL,
                     tuneGrid=mygrid,
                     trControl=trnCtrl,
              method="mlpKerasDropout",
              epochs=20)
saveRDS(model_RNDC, "9model_RNDC")
```

Visualización de los resultados del modelo:

```{r}
model_RNDC <- readRDS("/home/alumno25/ML/9model_RNDC")
model_RNDC
```

En esta salida se pueden observar los resultados de la validación cruzada del modelo de red neuronal multicapa con regularización de dropout en diferentes tamaños de capa oculta (size). La métrica utilizada para seleccionar el mejor modelo fue la exactitud (accuracy).Podemos observar que el valor más alto de exactitud se alcanza cuando el tamaño de la capa oculta es 6, con una exactitud de validación cruzada del 75.28% y un coeficiente kappa de 0.376. Además, los tamaños de capa oculta 2, 5 y 8 también tuvieron un rendimiento relativamente alto en términos de exactitud.Esta salida sugiere que una capa oculta de tamaño 6 puede ser adecuada para este conjunto de datos.

Para comparar cuantitativa y cualitativamente los modelos de la optimización neuronal con el de bosque aleatorio se pueden utilizar las métricas de evaluación de la clasificación como la precisión, la sensibilidad, la especificidad y el coeficiente kappa.

1.  Para comparar los modelos, se pueden realizar los siguientes pasos:

Como en el apartado del bosque aleatorio ya se ha realizado la matriz de confusión para este modelo,se procede a calcular la matriz de confusión para el modelo 9 de la optimización realizada previamente de la red neuronal.

```{r}
predicciones_RNDC <- predict(model_RNDC, newdata = DC_test)
levels(DC_test$resp.pCR) <- c("neg", "pos")
confusionMatrix(data = predicciones_RNDC, reference = DC_test$resp.pCR,positive = "pos")
```

```{r}
modeloRF <- list(Accuracy = 0.8214, Kappa = 0.375,Sensitivity=0.2857,Specificity=1.0000)
modeloRN <- list(Accuracy = 0.75, Kappa = 0.3636,Sensitivity=0.5714,Specificity=0.8095)

df <- data.frame(Modelo = c("Modelo RF", "Modelo RN"),
                 Accuracy = c(modeloRF$Accuracy, modeloRN$Accuracy),
                 Kappa = c(modeloRF$Kappa, modeloRN$Kappa),
                  Sensitivity = c(modeloRF$Sensitivity, modeloRN$Sensitivity),
                  Specificity = c(modeloRF$Specificity, modeloRN$Specificity))
df
```

Comparando los resultados de las matrices de confusión, se puede interpretar que el modelo de red neuronal optimizado tiene una precisión global del 75%, una sensibilidad del 57% y una especificidad del 80%, mientras que el modelo de bosque aleatorio tiene una precisión global del 82%, una sensibilidad del 28,5% y una especificidad del 100%. Además, el coeficiente kappa de la red neuronal es 0.36 y el del bosque aleatorio es 0.375.

Estos resultados indican que el modelo de bosque aleatorio tiene una especificidad perfecta (100%), lo que significa que no clasifica como positivos a casos negativos. Sin embargo, la sensibilidad es muy baja (29%), lo que indica que el modelo tiene dificultades para detectar los verdaderos positivos. Por otro lado, el modelo de red neuronal tiene una sensibilidad relativamente alta (57%), pero una especificidad más baja (80%), lo que indica que puede haber un mayor número de falsos positivos. En general, ambos modelos tienen un rendimiento moderado en la clasificación de la respuesta al tratamiento adyuvante

Para visualizar las comparaciones de las métricas de ambos modelos:

```{r}
df_largo <- pivot_longer(df, cols = c("Accuracy", "Kappa","Sensitivity","Specificity"), names_to = "Metrica", values_to = "Valor")

ggplot(df_largo, aes(x = Modelo, y = Valor, fill = Metrica)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("Accuracy" = "steelblue", "Kappa" = "darkred","Sensitivity" = "green","Specificity" = "orange")) +
  labs(title = "Comparación de modelos", y = "Valor", fill = "") +
  theme_minimal() +
  facet_wrap(~Metrica)
```

Según estos histogramas:

-   **Accuracy:** el modelo RF tiene un valor de 0.8214 y el modelo RN tiene un valor de 0.75. Por lo tanto, el modelo RF tiene una precisión ligeramente mayor que el modelo RN.

-   **Kappa:** el modelo RF tiene un valor de 0.375 y el modelo RN tiene un valor de 0.3636. Ambos modelos tienen valores similares de Kappa, lo que sugiere que tienen un nivel similar de concordancia en las predicciones.

-   **Sensitivity:** el modelo RF tiene un valor de 0.2857 y el modelo RN tiene un valor de 0.5714. El modelo RN tiene un valor mucho mayor de Sensitivity, lo que indica que tiene una mejor capacidad para detectar verdaderos positivos.

-   **Specificity:** el modelo RF tiene un valor de 1.0000 y el modelo RN tiene un valor de 0.8095. El modelo RF tiene un valor mucho mayor de Specificity, lo que indica que tiene una mejor capacidad para detectar verdaderos negativos.

En el caso de este estudio que busca determinar si la paciente ha respondido bien al tratamiento y, en consecuencia, no necesita someterse a cirugía, el modelo más adecuado sería aquel que maximice la sensibilidad del modelo, es decir, que sea capaz de detectar correctamente a los pacientes que necesitan cirugía.

En este caso, el modelo de red neuronal optimizado podría ser más adecuado, ya que tiene una sensibilidad del 57%, lo que indica que es capaz de detectar correctamente a más de la mitad de los pacientes que necesitan cirugía. Por otro lado, el modelo de bosque aleatorio tiene una sensibilidad muy baja del 28,5%, lo que significa que podría no detectar muchos de los pacientes que requieren cirugía.

Sin embargo, también es importante tener en cuenta que el modelo de red neuronal tiene una menor especificidad (80%) que el modelo de bosque aleatorio (100%), lo que significa que podría haber un mayor número de falsos positivos (pacientes que no necesitan cirugía pero son identificados por el modelo como tal). Por lo tanto, se deben considerar cuidadosamente los beneficios y riesgos de cada modelo antes de poder tomar la decisión de cual es el más adecuado para su aplicación clínica.

2.  Calcular la importancia de las características en cada modelo y compararlas.

Como en el apartado del bosque aleatorio ya se ha calculado las variables más importantes,se procede solo a determinar las variables más importantes para el modelo 9 de la optimización realizada previamente de la red neuronal.

```{r}
importancia_RNDC <- varImp(model_RNDC)
print(importancia_RNDC)
```

Antes de hablar sobre las variables importantes en cada uno de los modelos se debe de comentar que cada modelo de aprendizaje automático tiene su propia forma de priorizar las variables importantes.

En el caso de los bosques aleatorios, el modelo evalúa la importancia de las variables en función de la mejora que produce cada una de ellas en la medida de impureza (como el índice Gini o la entropía) en cada árbol. La importancia se calcula como la media del decremento de impureza en cada árbol que utiliza la variable. Por lo tanto, cuanto mayor sea la disminución en la impureza, mayor será la importancia de la variable.\
En el caso de la red neuronal, la importancia de las variables se determina a través del cálculo de los pesos o coeficientes asociados a cada variable. Los pesos se actualizan durante el proceso de entrenamiento de la red neuronal y su valor final refleja la importancia que tiene cada variable en la salida de la red. Si un peso tiene un valor grande, significa que la variable correspondiente tiene una gran influencia en la salida de la red.\
En general, los modelos de aprendizaje automático pueden tener distintas formas de evaluar la importancia de las variables, pero todas ellas tienen como objetivo identificar aquellas variables que tienen una mayor capacidad predictiva en la salida del modelo.

Al comparar la importancia de las variables en los distintos modelos se puede observar que la puntuación para las variables en la red neuronal son más altas estoo puede ser debido a que este modelo utiliza técnicas de aprendizaje profundo que le permiten encontrar patrones más complejos en los datos que el modelo de bosques aleatorios. Es decir, la red neuronal puede identificar relaciones no lineales entre las variables que el modelo de bosques aleatorios no puede capturar. Por lo tanto, las variables importantes para el modelo de la red neuronal pueden ser aquellas que contienen información crítica para identificar patrones sutiles en los datos y, por lo tanto, son más importantes para la predicción.

Por último, se puede comentar al comparar la impportancia de las variables que en el modelo de bosques aleatorios las variables más importantes son **"median_lymph_KDE_knn_50", seguida de "PGR.log2.tpm" y "Danaher.Neutrophils"**. Estas variables son las que tienen la mayor importancia para este modelo en términos de predecir la respuesta al tratamiento.Mientras que, en el modelo de red neuronal enfatiza en las variables **"PGR.log2.tpm", "ER.Allred","HRD.TelomericAI" ,"CIN.Prop" y "Danaher.Neutrophils"** como las más importantes para la predicción de la respuesta al tratamiento.

### Pregunta 10

**¿Aportan el análisis digital, la transcriptómica, o el perfil inmunológico, algo al conjunto de datos `Main.csv` con respecto a la capacidad para predecir respuesta al tratamiento? Describe detalladamente la metodología que vas a usar para responder a la pregunta, ejecuta dicha metodología y analiza los resultados, respondiendo a la pregunta para cada uno de los tres conjuntos por separado.**

Para poder responder a esta pregunta se procede a combinar los ficheros preprocesados con los datos clínicos (Main.csv) de forma separada, dando lugar a tres nuevos conjuntos de datos: datos_clínicos + analisis_digital, datos_clinicos + sistema_inmune y datos_clinicos + transcriptomica.Los datos que se combinan están escalados y sin valores de varianzas cercanas a 0.

1.Se eliminan las variables que son redundantes con la variables respuesta resp.pCR en el dataset de datos clínicos.

```{r}
DC_escalado$RCB.score=NULL
DC_escalado$resp.Chemoresistant=NULL
DC_escalado$resp.Chemosensitive=NULL
DC_escalado$RCB.category=NULL
```

2.  Se procede a unir los datasets filtrados de datos clínicos con análisis digital,sistema inmune y transcriptómica.Para ello se utiliza la función `merge()` entre estos conjuntos de datos , utilizando como clave de unión la columna "Trial.ID".

```{r}
DCAD1<-merge(DC_escalado,AD_escalado,by="Trial.ID")
DCSI1<-merge(DC_escalado,SI_escalado_filtrado,by="Trial.ID")
T_data<-as.data.frame(trans_var_filtrado_escalado)
Trial.ID<-Trial.ID_transDC
T_data<-cbind(trans_final,Trial.ID)
DCtrans1<-merge(DC_escalado,T_data,by="Trial.ID")
```

3.  Una vez que tenemos los conjuntos de datos combinados,tal y como se ha visto anteriormente, existen algunos predictores que presentan una alta correlación entre ellos y no va a interesar incluirlos en el modelo ya que van a proporcionar información redundante que no aporta nada nuevo.Para ello se crea una función que toma un conjunto de datos (data) y un umbral de correlación (cutoff) como argumentos. La función identifica las columnas numéricas en el conjunto de datos y calcula la matriz de correlación de estas columnas. Luego, utiliza la función `findCorrelation()` de la biblioteca caret para encontrar índices de columnas altamente correlacionadas (superiores al umbral cutoff). Después, se eliminan estas columnas del conjunto de datos original y se añade de nuevo la variable objetivo (resp.pCR) para el conjunto de datos filtrado.

```{r}
eliminar_varcor <- function(data, cutoff = 0.9) {
  col_num <- sapply(data, is.numeric)
  data_num <- data[, col_num]
  ind_cor <- findCorrelation(cor(data_num), cutoff = cutoff)
  data_sin_altcor <- data_num[,-ind_cor]
  data_sin_altcor$resp.pCR <- data$resp.pCR
  return(data_sin_altcor)
}
```

4.  Se aplica esta función a tres conjuntos de datos diferentes (DCAD1, DCSI1, DCtrans1) y se almacenan los conjuntos de datos filtrados (DCAD1_filtrado, DCSI1_filtrado, DCtrans1_filtrado) que tienen las columnas altamente correlacionadas eliminadas y la variable objetivo añadida de nuevo.

```{r}
DCAD1_filtrado <- eliminar_varcor(DCAD1)
DCSI1_filtrado <- eliminar_varcor(DCSI1)
DCtrans1_filtrado <- eliminar_varcor(DCtrans1)
resp.pCR<-DCtrans$resp.pCR
DCtrans1_filtrado<-cbind(DCtrans1_filtrado,resp.pCR)
```

5.  Para evaluar la capacidad predictiva de los modelos que se van a generar a continuación, es decir, comprobar cómo de próximas son sus predicciones a los verdaderos valores de la variable respuesta, se van a dividir los tres conjuntos de datos en un conjunto de entrenamiento y un conjunto de prueba.

```{r}
set.seed(456)
trainIndex_DCAD1 = createDataPartition(DCAD1_filtrado$resp.pCR,p=0.8,list = FALSE)
trainIndex_DCSI1 = createDataPartition(DCSI1_filtrado$resp.pCR,p=0.8,list = FALSE)
trainIndex_DCtrans1 = createDataPartition(DCtrans1_filtrado$resp.pCR,p=0.8,list = FALSE)
DCAD1_train = DCAD1_filtrado[trainIndex_DCAD1,]
DCAD1_test = DCAD1_filtrado[-trainIndex_DCAD1,]
DCSI1_train = DCSI1_filtrado[trainIndex_DCSI1,]
DCSI1_test = DCSI1_filtrado[-trainIndex_DCSI1,]
DCtrans1_train = DCtrans1_filtrado[trainIndex_DCtrans1,]
DCtrans1_test = DCtrans1_filtrado[-trainIndex_DCtrans1,]
```

6.  Entrenamiento de los conjuntos de datos creados a través de la implementación de bosques aleatorios.

Para ello se definen los parámetros para el control del proceso de entrenamiento de un modelo de clasificación mediante la función `trainControl()` del paquete caret.EL objeto de control utilizado es el denominado control_RF que hemos definido previamente en el apartado **2.2.2**

-   **datos clínicos + análisis digital**

El siguiente códiggo sirve para ajustar un modelo de random forest para predecir la variable resp.pCR a partir de todas las variables predictivas disponibles en el conjunto de datos DCAD1_train. Se utiliza la función `train()` del paquete caret, se especifica el método "ranger" para ajustar el modelo de random forest y se establecen las condiciones de validación cruzada definidas en el objeto control_RF. Además, se realiza una búsqueda aleatoria de hiperparámetros durante el proceso de ajuste del modelo (tuneLength = 30) y se calculan las importancias de las variables mediante la técnica de permutación (importance = "permutation"). El modelo ajustado se guarda en el objeto RFDCAD1.

```{r,eval=FALSE}
RFDCAD1 <- caret::train(resp.pCR~., data = DCAD1_train,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength=30,
                      importance = "permutation")
saveRDS(RFDCAD1,"RFDCAD1")
```

Visualización de los resultados:

```{r}
RFDCAD1<-readRDS("/home/alumno25/ML/RFDCAD1")
RFDCAD1
```

Esta salida corresponde con los resultados de la validación cruzada del modelo de Bosques Aleatorios (Random Forest) con el conjunto de datos DCAD1_train. Se muestra que se trabajó con 119 muestras y 43 variables predictoras, y que se utilizó una partición de 10-fold. Además, se realizaron pruebas con diferentes combinaciones de los hiperparámetros min.node.size, mtry y splitrule, y se evaluó el rendimiento del modelo según los criterios Accuracy y Kappa. En esta caso la mejor combinación de hiperparámetros fue mtry = 25, splitrule = gini y min.node.size = 2, que resultó en una exactitud (Accuracy) del modelo del 80.73% y un coeficiente kappa (Kappa) de 0.46.

\*Para los dos conjuntos restantes de datos el procedimiento será el mismo que con el conjunto de datos DCAD1 salvo que se usarán sus respectivos conjunto de entrenamiento y prueba.

-   **datos clínicos + sistema inmune**

1.  Entrenamiento del modelo.

```{r,eval=FALSE}
RFDCSI1 <- caret::train(resp.pCR~., data = DCSI1_train,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength=30,
                      importance = "permutation")
saveRDS(RFDCSI1,"RFDCSI1")
```

Visualización de los resultados del modelo.

```{r}
RFDCSI1 <-readRDS("/home/alumno25/ML/RFDCSI1")
RFDCSI1
```

Esta salida muestra los resultados del ajuste de un modelo de Random Forest mediante validación cruzada con 10 folds. El modelo se ha entrenado con 115 muestras y 45 características para predecir dos clases: 0 y 1. La validación cruzada se ha realizado sin ningún preprocesamiento adicional. Se han evaluado diferentes combinaciones de hiperparámetros (min.node.size, mtry y splitrule) y se ha utilizado la precisión (Accuracy) para seleccionar el mejor modelo. En este caso los valores óptimos para el modelo son mtry = 22, splitrule = gini y min.node.size = 18. Con estos valores de hiperparámetros se obtiene un modelo con una exactitud (accuracy) del 0.7893939, lo que significa que aproximadamente el 78.94% de las veces puede clasificar correctamente si un paciente tendrá o no una respuesta completa patológica. El kappa de 0.4430943 indica que el modelo tiene una adecuada capacidad de predicción.

-   **datos clínicos + transcriptómica**

1.  Entrenamiento del modelo.

```{r,eval=FALSE}
RFDCtrans1 <- caret::train(resp.pCR~., data = DCtrans1_train,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength=30,
                      importance = "permutation")
saveRDS(RFDCtrans1,"RFDCtrans1")
```

Visualización de los resultados del modelo.

```{r}
RFDCtrans1 <-readRDS("/home/alumno25/ML/RFDCtrans1")
RFDCtrans1
```

Esta salida muestra que el modelo de Random Forest que utiliza 146 predictores para clasificar entre dos clases ('0' y '1'). Se utilizó validación cruzada de 10 folds para evaluar el desempeño del modelo. En este caso, el modelo con el mayor valor de accuracy (0.7668415) y kappa (0.19874687) utilizó un tamaño mínimo de nodo terminal de 13, mtry de 145 y el método de división de gini. En general, estos valores indican que el modelo tiene un rendimiento moderado, con una precisión del 76.68% en la clasificación de las observaciones.Además, tiene un valor de 0.198 de kappa.

Para determinar si los ficheros de análisis digital, transcriptómica o perfil inmunológico aportan algo al conjunto de datos Main.csv en términos de capacidad predictiva, se procede a hacer una comparación del rendimiento de los modelos entrenados con los conjuntos combinados (DCAD1, DCSI1 y DCtrans1) con el modelo entrenado utilizando solo el conjunto de datos Main.csv. Si los modelos combinados tienen un rendimiento significativamente mejor que el modelo basado solo en Main.csv, entonces podría sugerir que los conjuntos de datos adicionales aportan información útil para predecir la respuesta al tratamiento.

1.  Se crea una función llamada "evaluar_rendimiento" que toma como argumentos un modelo entrenado y un conjunto de datos de prueba, y devuelve un diccionario que contiene varias métricas de rendimiento del modelo en los datos de prueba.Esta la función usa el modelo para hacer predicciones sobre los datos de prueba y almacena las predicciones en la variable "prediccion". Luego, utiliza la función `confusionMatrix()` de la librería "caret" para calcular la matriz de confusión del modelo. A continuación, la función extrae varias métricas de rendimiento de la matriz de confusión y las almacena en un diccionario llamado "metricas". Las métricas que se calculan son la precisión (Accuracy), la sensibilidad (Sensitivity), la especificidad (Specificity) y el coeficiente kappa (Kappa).Por último, la función devuelve el diccionario de métricas.

```{r}
evaluar_rendimiento <- function(modelo, datos_test) {
  prediccion = predict(modelo, datos_test)
  matriz_confusion = confusionMatrix(prediccion, datos_test$resp.pCR,positive = "1")
  metricas = list(Accuracy = matriz_confusion$overall["Accuracy"],
                  Sensitivity = matriz_confusion$byClass["Sensitivity"],
                  Specificity = matriz_confusion$byClass["Specificity"],
                  Kappa = matriz_confusion$overall["Kappa"])
  return(metricas)
}

```

2.  Llamada a la función anterior con los distintos conjuntos de datos.

```{r}
rendimiento_DCAD1 = evaluar_rendimiento(RFDCAD1, DCAD1_test)
rendimiento_DCSI1 = evaluar_rendimiento(RFDCSI1, DCSI1_test)
rendimiento_DCtrans1 = evaluar_rendimiento(RFDCtrans1, DCtrans1_test)
```

3.  A continuación, se crea un data frame que contiene las métricas para cada modelo entrenado y así poder hacer una comparación del rendimiento de diferentes modelos de aprendizaje automático (Random Forest) entrenados en cuatro conjuntos de datos diferentes (DC,DCAD1, DCSI1, y DCtrans1).

```{r}
rendimiento_DCAD1_df <- data.frame(Metrica = names(rendimiento_DCAD1), DCAD1 = unlist(rendimiento_DCAD1))
rendimiento_DCSI1_df <- data.frame(Metrica = names(rendimiento_DCSI1), DCSI1 = unlist(rendimiento_DCSI1))
rendimiento_DCtrans1_df <- data.frame(Metrica = names(rendimiento_DCtrans1), DCtrans1 = unlist(rendimiento_DCtrans1))
comparacion_rendimiento <- Reduce(function(x, y) merge(x, y, by = "Metrica", all = TRUE),
                                  list(rendimiento_DCAD1_df, rendimiento_DCSI1_df, rendimiento_DCtrans1_df))

DC <- data.frame(Metrica = c("Accuracy", "Kappa", "Sensitivity", "Specificity"),
                   DC = c(0.8214, 0.375, 0.2871, 1.000))

comparacion_rendimiento <- merge(comparacion_rendimiento, DC, by = "Metrica", all = TRUE)
comparacion_rendimiento
```

Los resultados de esta table muestran:

-   El **modelo DCAD1 (datos clínicos y análisis digital)** tiene la mayor precisión general (accuracy) de todos los modelos, lo que sugiere que la adición de datos de análisis digital mejora la capacidad predictiva del modelo. La sensibilidad de este modelo es del 57%, lo que significa que puede identificar correctamente el 57% de los casos que tienen una respuesta positiva al tratamiento. La especificidad de este modelo es del 100%, lo que significa que puede identificar correctamente el 100% de los casos que no tienen una respuesta positiva al tratamiento.

-   El **modelo DCSI1 (datos clínicos y sistema inmune)** tiene una precisión general más baja, pero una especificidad relativamente alta del 85,7%, lo que significa que puede identificar correctamente el 85,7% de los casos que no tienen una respuesta positiva al tratamiento. Sin embargo, la sensibilidad de este modelo es baja, del 28,6%, lo que significa que solo puede identificar correctamente el 28,6% de los casos que tienen una respuesta positiva al tratamiento.

-   El **modelo DCtrans1 (datos clínicos y transcriptómica)** tiene una precisión general intermedia y una especificidad del 100%, lo que significa que puede identificar correctamente el 100% de los casos que no tienen una respuesta positiva al tratamiento. Sin embargo, la sensibilidad de este modelo es baja, del 28,6%, lo que significa que solo puede identificar correctamente el 28,6% de los casos que tienen una respuesta positiva al tratamiento.

En general, estos resultados sugieren que la adición de datos de análisis digital puede mejorar la capacidad predictiva del modelo y que la adición de datos de sistema inmunológico o transcriptómica pueden no ser tan útiles.

4.  Visualización de las métricas de los modelos.

```{r}
datos_RF <- data.frame(Modelo = c("DCAD1", "DCSI1", "DCtrans1", "DC"),
                    Accuracy = c(0.8928571, 0.7142857, 0.8214286, 0.8214),
                    Kappa = c(0.6666667, 0.1578947, 0.3750000, 0.3750),
                    Sensitivity = c(0.5714286, 0.2857143, 0.2857143, 0.2871),
                    Specificity = c(1.0000000, 0.8571429, 1.0000000, 1.0000))
```

-   **Accuracy:**proporción de casos clasificados correctamente.

```{r}

ggplot(datos_RF, aes(x = Modelo, y = Accuracy)) + 
  geom_bar(stat = "identity", fill = "#99d8c9") +
  labs(x = "Modelo", y = "Accuracy") +
  ggtitle("Histograma de Accuracy")+
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))

```

En el modelo DCAD1, se obtiene un valor alto de 0.8928, lo que indica que es capaz de predecir correctamente la respuesta al tratamiento en la mayoría de los casos. En el modelo DCSI1, se obtiene un valor menor de 0.7143, lo que indica que tiene una menor capacidad para predecir correctamente la respuesta al tratamiento. En el modelo DCtrans1, se obtiene un valor intermedio de 0.8214, lo que sugiere que tiene una capacidad aceptable para predecir la respuesta al tratamiento. En el modelo DC, también se obtiene un valor intermedio de 0.8214, lo que indica que su capacidad para predecir la respuesta al tratamiento es similar al modelo DCtrans1.

-   **Kappa:** medida que ajusta la proporción de casos clasificados correctamente por el azar.

```{r}
ggplot(datos_RF, aes(x = Modelo, y = Kappa)) + 
  geom_bar(stat = "identity", fill = "#ffc300") +
  labs(x = "Modelo", y = "Kappa") +
  ggtitle("Histograma de Kappa") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))
```

En el modelo DCAD1, se obtiene un valor alto de 0.6667, lo que indica que su capacidad de predicción es mejor que la que se esperaría por azar. En el modelo DCSI1, se obtiene un valor muy bajo de 0.1579, lo que sugiere que su capacidad de predicción es similar a la que se obtendría por azar. En el modelo DCtrans1, se obtiene un valor intermedio de 0.3750, lo que indica que su capacidad de predicción es mejor que la que se esperaría por azar pero no tan buena como en el modelo DCAD1. En el modelo DC, se obtiene un valor intermedio similar al modelo DCtrans1 de 0.3750.

-   **Sensitivity:** proporción de verdaderos positivos (pacientes que responden al tratamiento y son clasificados correctamente).

```{r}
ggplot(datos_RF, aes(x = Modelo, y = Sensitivity)) + 
  geom_bar(stat = "identity", fill = "#6a5acd") +
  labs(x = "Modelo", y = "Sensitivity") +
  ggtitle("Histograma de Sensitivity") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))

```

En los modelos presentados, la sensibilidad varía significativamente entre los diferentes modelos. En el modelo DCAD1, la sensibilidad es del 57.14%, lo que significa que el modelo puede identificar correctamente el 57.14% de los casos positivos. En el modelo DCSI1, la sensibilidad es del 28.57%, lo que indica que el modelo es menos efectivo para detectar correctamente los casos positivos. En el modelo DCtrans1, la sensibilidad es del 28.57%, lo que indica que el modelo tiene dificultades para identificar correctamente los casos positivos. En el modelo DC, la sensibilidad es del 28.71%, lo que indica que el modelo también tiene dificultades para detectar correctamente los casos positivos.

-   **Specifity:** capacidad del modelo para detectar correctamente los casos negativos.

```{r}
ggplot(datos_RF, aes(x = Modelo, y = Specificity)) + 
  geom_bar(stat = "identity", fill = "#ff7f50") +
  labs(x = "Modelo", y = "Specificity") +
  ggtitle("Histograma de Specificity") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))
```

En los modelos presentados, la especificidad es del 100% en tres de ellos: DCAD1, DCtrans1 y DC. Esto significa que estos modelos son muy efectivos para detectar correctamente los casos negativos. En el modelo DCSI1, la especificidad es del 85.71%, lo que indica que el modelo tiene dificultades para detectar correctamente los casos negativos en comparación con los otros modelos.

### Pregunta 11

**¿Qué ocurre si fusionamos todos los conjuntos de datos en uno solo? ¿Existe alguna técnica que reduzca la dimensionalidad del conjunto, aun a riesgo de perder interpretabilidad? Si es así, comprueba si funciona o por el contrario no ganamos nada. ¿Y si queremos mantener la interpretabilidad intacta? ¿Se te ocurre alguna técnica para seleccionar los predictores relevantes? Si es así, evalúala y comenta los resultados.**

Para poder responder a la pregunta de **"¿Qué ocurre si fusionamos todos los conjuntos de datos en uno solo?"** se procede a fusionar los cuatro conjuntos de datos en uno solo utilizando la columna "Trial.ID" con la función `merge()`.Los conjuntos de datos estan escalados y con las variables con varianza cercana a 0 eliminadas.

\*Cabe añadir que el conjunto de datos de transcriptómica esta procesado con un análisis de componentes principales ya que ha sido la manera en la que se ha podido reducir el número de variables a analizar en el modelo y evitar el error de stack overflow que se producía.

```{r}
primero<-merge(DC_escalado,AD_escalado,by="Trial.ID")
segundo<-merge(primero,SI_escalado_filtrado,by="Trial.ID")
DAST<-merge(segundo,T_data,by="Trial.ID")
```

Para saber cuantas variables numéricas, categóricas y de tipo caracter tenemos en el conjunto de datos fusionao se realiza el siguiente código:

```{r}
num_vars <- sum(sapply(DAST, is.numeric))
cat_vars <- sum(sapply(DAST, is.factor))
num_vars_caracteres <- sum(sapply(DAST, is.character))

cat("El conjunto de datos fusionado tiene", num_vars, "variables numéricas, ", cat_vars, "variables categóricas y ",num_vars_caracteres, "variable de tipo caracter")
```

3.  Una vez que tenemos los conjuntos de datos combinados,tal y como se ha visto anteriormente, existen algunos predictores que presentan una alta correlación entre ellos y no va a interesar incluirlos en el modelo ya que van a proporcionar información redundante que no aporta nada nuevo.Para ello utiliza la función eliminar_varcor definida en el apartado anterior(2.2.4)

```{r}
DAST_filtrado <- eliminar_varcor(DAST)
resp.pCR<-DAST$resp.pCR.x
DAST_filtrado<-cbind(DAST_filtrado,resp.pCR)
```

4.Una vez eliminadas las variables altamente correlacionadas, se prodece a dividir el conjunto de datos en conjunto de entrenamiento y prueba para evaluar la capacidad predictiva del modelo que se generará en el posterior paso.

```{r}
set.seed(456)
trainIndex_DAST = createDataPartition(DAST_filtrado$resp.pCR,p=0.8,list = FALSE)
DAST_train = DAST_filtrado[trainIndex_DAST,]
DAST_test = DAST_filtrado[-trainIndex_DAST,]
```

5.  Entrenamiento de los conjuntos de datos creados a través de la implementación de bosques aleatorios.Para ello se definen los parámetros para el control del proceso de entrenamiento de un modelo de clasificación mediante la función `trainControl()` del paquete caret.EL objeto de control utilizado es el denominado control_RF que hemos definido previamente en el apartado **2.2.2**.Para entrenar el modelo se usa el conjunto de datos DAST_train. Se utiliza el método de clasificación "ranger"y una la validación cruzada con 10 particiones (trControl = control_RF) y se ajusta el modelo con 30 combinaciones diferentes de hiperparámetros (tuneLength=30). Por último, se especifica que se calculen las importancias de las variables mediante la técnica de permutación (importance = "permutation").

```{r,eval=FALSE}
RFDAST <- caret::train(resp.pCR~., data = DAST_train,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength=30,
                      importance = "permutation")
saveRDS(RFDAST,"RFDAST")
```

6.  Visualización de los resultados del modelo:

```{r}
RFDAST <-readRDS("/home/alumno25/ML/RFDAST")
RFDAST
```

Esta salida muestra la información relevante del modelo Random Forest (RF).Este modelo ha sido entrenado usando los datos de DAST_train con la variable objetivo resp.pCR. El modelo utiliza 161 variables predictoras y tiene 115 muestras.Además, tambien se observan los resultados de la validación cruzada 10-fold para distintas combinaciones de hiperparámetros (min.node.size, mtry y splitrule), donde se ha usado la métrica de exactitud (Accuracy) para seleccionar el mejor modelo, que resultó tener un valor de mtry de 155, splitrule de gini ,min.node.size de 20, valor de exactitud de 0.7636 y valor de kappa de 0.293. En general, el modelo Random Forest con los parámetros especificados es el mejor modelo que se puede utilizar para este conjunto de datos, ya que logra una precisión razonablemente alta y una concordancia moderada entre las predicciones y los valores reales.

7.  Evaluación de la capacidad predictiva usando la matriz de confusión.

```{r}
predicciones_RFDAST <- predict(RFDAST, newdata = DAST_test)
confusionMatrix(data = predicciones_RFDAST, reference = DAST_test$resp.pCR,positive = "1")
```

Esta salida muestra:

-   La **matriz de confusión** muestra cuatro resultados posibles: verdaderos positivos (TP), falsos positivos (FP), verdaderos negativos (TN) y falsos negativos (FN). En este caso, la matriz de confusión muestra que hubo 21 verdaderos negativos (TN), 4 falsos negativos (FN), 0 falsos positivos (FP) y 3 verdaderos positivos (TP).

-   La **precisión (Accuracy)** del modelo fue del 0.8571, lo que indica que el modelo predijo correctamente el 85,71% de los resultados.

-   El **valor de Kappa**, una medida de la precisión de la predicción del modelo en relación a una predicción aleatoria, fue de 0.5294, lo que sugiere una precisión moderada del modelo.

-   La **sensibilidad (Sensitivity) y especificidad (Specificity)**, que indican la proporción de resultados positivos y negativos correctamente identificados por el modelo, respectivamente. En este caso, la sensibilidad del modelo fue de 0.4286 y la especificidad del modelo fue de 1.0000.

En general, aunque la precisión del modelo es aceptable, los valores de sensibilidad y Kappa sugieren que hay margen de mejora en la capacidad predictiva del modelo.

8.  Comparación de la capacidad predictiva de analizar los conjuntos de datos por separados y fusionados

```{r}
tabla_rendimiento <- data.frame(
  Métrica = c("Accuracy", "Kappa", "Sensitivity", "Specificity"),
  DAST = c(0.8571, 0.5294, 0.4286, 1.0000),
  DC = c(0.8214, 0.3750, 0.2857, 1.0000),
  AD = c(0.7500, 0.1250, 0.1429, 0.9524),
  SI = c(0.6429, -0.0526, 0.1429, 0.8095),
  trans = c(0.7857, 0.4286, 0.5714, 0.8571)
)

print(tabla_rendimiento)

```

Según la información de esta tabla, el modelo fusionado de los cuatro conjuntos de datos (DAST) parece tener un rendimiento mejor que los modelos de los conjuntos de datos separados. La métrica Accuracy en el modelo fusionado es la más alta, lo que sugiere que es el modelo más preciso en términos generales. Además, el valor de kappa también es alto en comparación con los modelos individuales, lo que indica una buena concordancia entre las predicciones del modelo y las observaciones reales.

En cuanto a las métricas de sensibilidad y especificidad, el modelo DAST tiene una especificidad perfecta, lo que significa que es muy bueno en la detección de verdaderos negativos, mientras que los modelos AD, SI y trans tienen especificidades menores pero aún así decentes. En cuanto a la sensibilidad, el modelo DAST tiene el valor más alto, seguido por el modelo trans, mientras que los modelos DC y AD tienen sensibilidades mucho más bajas.

En general, el modelo fusionado de los cuatro conjuntos de datos parece ser una buena opción en términos de rendimiento general y puede superar a los modelos individuales en algunas métricas. Sin embargo, es importante tener en cuenta que cada conjunto de datos individual puede tener información valiosa y que el modelo fusionado podría estar ocultando algunas diferencias importantes entre los conjuntos de datos.

Para visualizar las comparaciones de las métricos se realizan los siguientes gráficos:

```{r}
tabla_rendimiento_largo <- gather(tabla_rendimiento, key = "Modelo", value = "Valor", -Métrica)
```

-   **Accuracy**

```{r}
ggplot(subset(tabla_rendimiento_largo, Métrica == "Accuracy"),
       aes(x = Modelo, y = Valor, fill = Modelo)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparación de Accuracy de los modelos",
       x = "Modelo", y = "Valor") +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))

```

En términos de precisión (Accuracy), el modelo DAST presenta el mejor desempeño,es decir, es capaz de realizar las predicciones más precisas y consistentes en comparación con los otros modelos.En segundo lugar se encuentra el modelo DC y luego el modelo trans. Los modelos AD y SI presentan las peores precisiones.

-   **Kappa**

```{r}
ggplot(subset(tabla_rendimiento_largo, Métrica == "Kappa"),
       aes(x = Modelo, y = Valor, fill = Modelo)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparación de Kappa de los modelos",
       x = "Modelo", y = "Valor") +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(-0.25, 1), breaks = seq(0, 1, by = 0.25))
```

En cuanto a la capacidad de concordancia (Kappa), el modelo DAST también presenta el mejor desempeño, seguido por el modelo trans y luego por el modelo DC. Los modelos AD y SI presentan los valores más bajos de Kappa.

-   **Sensitivity**

```{r}
ggplot(subset(tabla_rendimiento_largo, Métrica == "Sensitivity"),
       aes(x = Modelo, y = Valor, fill = Modelo)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparación de Sensitivity de los modelos",
       x = "Modelo", y = "Valor") +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))
```

En términos de sensibilidad (Sensitivity), el modelo trans presenta el mejor desempeño, seguido por el modelo DAST. Los modelos DC, AD y SI presentan valores más bajos de sensibilidad.

-   **Specificity**

```{r}

ggplot(subset(tabla_rendimiento_largo, Métrica == "Specificity"),
       aes(x = Modelo, y = Valor, fill = Modelo)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparación de Specificity de los modelos",
       x = "Modelo", y = "Valor") +
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))
```

En cuanto a la especificidad (Specificity), los modelos DAST y DC presentan los mejores desempeños, seguidos por el modelo AD. Los modelos trans y SI presentan valores más bajos de especificidad.

9.  Implementación la función `varImp()` del paquete caret en el modelo entrenado RFDAST.Esta es una técnica para evaluar la importancia de las variables predictoras y reducir la dimensionalidad del conjunto de datos. Esto se logra eliminando las variables que tienen una baja importancia en la predicción de la variable objetivo, lo que puede reducir la complejidad del modelo y mejorar su rendimiento. Sin embargo, es importante tener en cuenta que esta técnica no garantiza la interpretabilidad de las variables seleccionadas, ya que la importancia se evalúa en función de la contribución del predictor al modelo en su conjunto, no necesariamente a su relación causal con la variable objetivo.

```{r}
varImpRFDAST <- varImp(RFDAST)
varImpRFDAST
```

Según esta salida se procede a realizar la elección de qué variables quitar. Dado que el resultado de la evaluación de las variables mediante varImp en el modelo RFDAST es una lista de importancia de las variables, podría ser una buena idea eliminar las variables con baja importancia relativa para reducir la dimensionalidad del conjunto de datos y, por lo tanto, reducir el riesgo de sobreajuste del modelo.

Se proceden a eliminar las variables con una importancia menor a 9, por ejemplo, como **"Danaher.Mast.cells", "Danaher.Neutrophils", "PC88", "Swanton.PaclitaxelScore", "STAT1.gsva", "PC90", "PC22", "PC2", "Danaher.NK.cells","Age.at.diagnosis" y "HRD.LOH"**.

10. Se utiliza la función `subset()` para seleccionar únicamente las variables que no se quiere eliminar, creando así un nuevo conjunto de datos que contenga solo las variables que deseas mantener denominado DAST_filtrado_reducido.

```{r}
DAST_filtrado_reducido <- subset(DAST_filtrado, select = c("ESR1.log2.tpm", "median_lymph_KDE_knn_50.y", "PGR.log2.tpm", "PC19", "PC58", "ER.Allred", "HRD.LST", "fraction_lymph", "HRD.sum"))
resp.pCR<- DAST_filtrado$resp.pCR
DAST_filtrado_reducido <- cbind(DAST_filtrado_reducido,resp.pCR)
```

11. Entrenar un nuevo modelo utilizando solo las variables seleccionadas. Para ello se procede a hacer un nuevo conjunto de datos de entrenamiento y test para las variables seleccionadas.

```{r}
set.seed(456)
trainIndex_DAST_reducido = createDataPartition(DAST_filtrado_reducido$resp.pCR, p = 0.8, list = FALSE)
DAST_train_reducido = DAST_filtrado_reducido[trainIndex_DAST_reducido,]
DAST_test_reducido = DAST_filtrado_reducido[-trainIndex_DAST_reducido,]
```

Entrenamiento del modelo.

```{r,eval=FALSE}
RFDAST_reducido <- caret::train(resp.pCR ~ ., data = DAST_train_reducido,
                      method = "ranger",
                      trControl = control_RF,
                      tuneLength = 30,
                      importance = "permutation")
saveRDS(RFDAST_reducido,"RFDAST_reducido")
```

Visualización de los resultados del modelo.

```{r}
RFDAST_reducido<-readRDS("/home/alumno25/ML/RFDAST_reducido")
RFDAST_reducido
```

La salida muestra diferentes resultados de resampling para diferentes valores de los hiperparámetros del modelo (mtry, splitrule, min.node.size). La combinación de hiperparámetros que proporcionó la precisión más alta se seleccionó como el modelo óptimo. En este caso, los hiperparámetros óptimos fueron mtry = 7, splitrule = gini y min.node.size = 13, lo que resultó en una precisión del 84,3% y una kappa de 0,57.

12. Evaluación el rendimiento del nuevo modelo.

```{r}
predicciones_RFDAST_reducido <- predict(RFDAST_reducido, newdata = DAST_test_reducido)
confusionMatrix(data = predicciones_RFDAST_reducido, reference = DAST_test_reducido$resp.pCR, positive = "1")
```

La salida muestra una matriz de confusión que presenta los resultados de la predicción del modelo. En este caso, el modelo ha predicho correctamente 19 de 22 observaciones de la clase 0 y 4 de 7 observaciones de la clase 1. La exactitud (Accuracy) del modelo es del 82.14%, lo que significa que el modelo ha predicho correctamente el 82.14% de las observaciones en el conjunto de prueba. La sensibilidad (Sensitivity) del modelo es del 57.14%, lo que significa que el modelo ha identificado correctamente el 57.14% de las observaciones positivas (clase 1) del conjunto de prueba. La especificidad (Specificity) del modelo es del 90.48%, lo que significa que el modelo ha identificado correctamente el 90.48% de las observaciones negativas (clase 0) del conjunto de prueba. El valor de Kappa de 0.5 indica una concordancia moderada entre las predicciones del modelo y las observaciones reales. En general, el modelo parece tener una precisión razonable pero podría mejorarse.

13. Comparación del modelo RFDAST y RFDAST_reducido.

```{r}
tabla_rendimiento2 <- data.frame(
  Métrica = c("Accuracy", "Kappa", "Sensitivity", "Specificity"),
  DAST = c(0.8571, 0.5294, 0.4286, 1.0000),
  DAST_reducido = c(0.8214,0.5,0.5714,0.9048)
)

print(tabla_rendimiento2)
```

De los resultados de esta tabla, se puede observar que la eliminación de las variables menos importantes con la técnica varImp ha disminuido ligeramente la capacidad del modelo para predecir la variable objetivo. El modelo DAST original tenía una precisión del 85,71% mientras que el modelo reducido tenía una precisión del 82,14%. Del mismo modo, la capacidad del modelo para predecir la variable objetivo también ha disminuido ligeramente en términos de Kappa y Specificity, mientras que la sensibilidad ha aumentado de valor. La interpretación de la salida sugiere que la eliminación de las variables menos importantes utilizando la técnica varImp ha tenido un impacto ligeramente negativo en la capacidad predictiva del modelo en términos de precisión, Kappa y especificidad. Sin embargo, la sensibilidad ha mejorado. Por lo tanto, hay un equilibrio entre la reducción de la dimensionalidad del conjunto de datos y la pérdida de precisión en la capacidad predictiva del modelo. Cabe destacar que siempre hay un riesgo asociado de perder información importante al eliminar variables.

Para visualizar las diferencias en las métricas de ambos modelos se realiza el siguiente código:

```{r}
tabla_rendimiento2_long <- pivot_longer(tabla_rendimiento2, -Métrica)
ggplot(tabla_rendimiento2_long, aes(x = Métrica, y = value, fill = name)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Métrica", y = "Valor", fill = "Modelo") +
  ggtitle("Comparación de rendimiento entre modelos") +
  theme_minimal()
```

Con esta gráfica se puede verificar de manera visual, lo mencionado anteriormente.

14. Eliminación recursiva de variables.

La eliminación recursiva de variables es una técnica de selección de características que utiliza un modelo de aprendizaje automático para identificar la combinación óptima de características que mejor se ajusta a los datos. En este enfoque, el modelo se entrena inicialmente con todas las características y luego se van eliminando iterativamente las características menos importantes, hasta que se obtiene la combinación óptima. La importancia de cada característica se determina mediante la evaluación de su impacto en el rendimiento del modelo a medida que se van eliminando. Al final de cada iteración, se evalúa el rendimiento del modelo y se eliminan las características menos importantes hasta que se alcanza la combinación óptima de características. Este método puede ser muy útil para seleccionar un subconjunto de características relevantes para un modelo de aprendizaje automático.

Para realizar este punto se usará el conjunto de datos DAST_filtrado ya que este esta escalado, sin varianzas cercanas a 0 y con las variables altamente correlacionadas eliminadas.

Se crea una lista de semillas aleatorias para su uso en análisis de selección de características. Además,define un rango de tamaños de subconjuntos de variables de 3 a 40, luego usa la función `sample.int()` para generar 10 semillas aleatorias únicas de 1000 valores para cada tamaño de subconjunto más uno adicional, y agrega una undécima semilla aleatoria única de 1000 valores para su uso posterior en el análisis de validación cruzada. El uso de estas semillas aleatorias ayuda a garantizar que el proceso de selección de características se realice de manera consistente y reproducible en múltiples ejecuciones del código.

```{r}
subsets <- c(3:40)
set.seed(456)
seeds <- vector(mode = "list", length = 6)
for(i in 1:10) seeds[[i]] <- sample.int(1000, length(subsets) + 1)
seeds[[11]] <- sample.int(1000, 1)
```

A continuación, se genera un objeto de control para la selección recursiva de características (RFE) utilizando la función `rfeControl()` de la biblioteca "caret". En el objeto de control, se especifican los métodos de clasificación (en este caso, "treebagFuncs"), el método de validación cruzada (CV), el número de pliegues de CV (10), las semillas para la generación aleatoria y el detalle de los resultados devueltos (en este caso, el resultado final). Además, el parámetro "verbose" se establece en "TRUE" para mostrar información detallada durante la ejecución de la RFE.

La técnica treebagFuncs es una técnica de selección de características que utiliza un modelo de ensamble conocido como Random Forest, que se basa en la construcción de múltiples árboles de decisión independientes. En esta técnica, se genera una muestra aleatoria con reemplazo del conjunto de datos original para construir cada uno de los árboles, lo que garantiza la diversidad en el conjunto de modelos. La selección de características se realiza mediante la importancia de las variables de la muestra de entrenamiento, que se calcula como el incremento promedio en el error de clasificación al excluir la variable en cuestión de los árboles de decisión. La técnica treebagFuncs utiliza el método de Bagging para construir los árboles y el índice de Gini para medir la pureza de las ramas del árbol. La técnica es útil para problemas de clasificación y regresión.

```{r}
set.seed(456)
ctrl_treebag_rfe_RFDAST <- rfeControl(functions=treebagFuncs, 
                                           method = "cv", 
                                           number = 10,
                                           seeds = seeds,
                                           returnResamp="final", 
                                           verbose = TRUE)
```

El siguiente código realiza una eliminación recursiva de variables (RFE) utilizando el algoritmo de bolsa de árboles (treebag) para predecir la respuesta pCR. Para ello, se utiliza la función `rfe()` de la librería caret. Se especifica la variable respuesta "resp.pCR" y todas las variables predictoras mediante "\~." y se proporciona el conjunto de datos "DAST_filtrado". Se especifica la variable "subsets" que contiene los tamaños de los conjuntos de variables que se van a evaluar en la selección de variables. Finalmente, se especifica el objeto de control creado previamente con la función rfeControl.

```{r,eval=FALSE}
treebag_rfe_RFDAST <- rfe(resp.pCR~., data=DAST_filtrado,
                         sizes=subsets, 
                         rfeControl=ctrl_treebag_rfe_RFDAST)
saveRDS(treebag_rfe_RFDAST,"treebag_rfe_RFDAST")
```

El objeto treebag_rfe_RFDAST,contiene la información de las variables seleccionadas y la evaluación del modelo en cada iteración.

```{r}
treebag_rfe_RFDAST<-readRDS("/home/alumno25/ML/treebag_rfe_RFDAST")
```

Se crea un data.frame que almacena la información del mejor subconjunto de variables que se ha encontrado mediante la técnica de eliminación recursiva de variables (RFE) utilizando el método treebagFuncs en el modelo Random Forest.

```{r}
data.frame("Accuracy" = c(max(treebag_rfe_RFDAST$results$Accuracy)),
                            
                 "Kappa" = c(max(treebag_rfe_RFDAST$results$Kappa)),
                 
                 "Predictors" = c(treebag_rfe_RFDAST$bestSubset))
```

De esta salida se observa que el dataframe tiene tres columnas: "Accuracy" (exactitud), "Kappa" (coeficiente kappa) y "Predictors" (predictores). La columna "Accuracy" muestra la mayor precisión encontrada, la columna "Kappa" muestra el valor máximo del coeficiente kappa y la columna "Predictors" almacena el subconjunto de variables que proporcionó la mayor precisión y el valor máximo de kappa. En este caso, el mejor subconjunto de variables incluye 16 predictores,el modelo generado a partir de estos predictores tiene una exactitud del 81.19% y un valor de kappa de 0.4497, lo que indica un acuerdo moderado con las observaciones reales.

Con el siguiente código se extraen las variables óptimas seleccionadas por la técnica Recursive Feature Elimination (RFE) utilizando el modelo generado por la técnica treebagFuncs. La variable rfe_DAST guarda una lista de índices que corresponden a las variables seleccionadas.

```{r}
rfe_DAST <- treebag_rfe_RFDAST$optVariables
rfe_DAST
```

Es posible que estas variables sean importantes en un estudio de cáncer de mama, ya que muchas de ellas están relacionadas con los marcadores moleculares y características clínicas comunes de los tumores de mama. Por ejemplo, ESR1 y PGR son los receptores hormonales más comunes en los tumores de mama y, por lo tanto, son importantes en la evaluación del pronóstico y tratamiento de estos tumores. De manera similar, ERBB2 (también conocido como HER2) es un receptor de superficie celular que se encuentra en algunas células de cáncer de mama y es un objetivo terapéutico importante. Además, la edad en el momento del diagnóstico también es un factor importante a considerar en la evaluación del pronóstico y tratamiento del cáncer de mama.

15. Una vez que los predictores han sido seleccionados, el siguiente paso es emplear un algoritmo para entrenar y evaluar el modelo.Para ello se necesita obtener un conjunto de entrenamiento y un conjunto de test para evaluar la capacidad predictiva de los modelos que se van a obtener. Estos conjuntos se van a crear introduciendo en cada uno de ellos únicamente los predictores seleccionados y la columna resp.pCR.

```{r}
predictors_DAST = c(rfe_DAST,"resp.pCR")
```

Creacción de los conjuntos de entrenamiento y prueba.Se crean dos nuevos dataframes: DAST_train_pred y DAST_test_pred. Ambos son subconjuntos del dataframe original DAST_train y DAST_test, respectivamente. Los subconjuntos se crean seleccionando solo las columnas (predictores) que se han identificado previamente como importantes en el análisis de selección de variables. Esto permite que los nuevos dataframes solo contengan las variables más relevantes para predecir la variable objetivo, lo que puede mejorar la capacidad predictiva del modelo y reducir el ruido de variables irrelevantes.

```{r}
DAST_train_pred = DAST_train[,predictors_DAST]
DAST_test_pred = DAST_test[,predictors_DAST]
```

Se procede a definir los parámetros de control para el proceso de entrenamiento y validación cruzada del modelo. La función `trainControl()` del paquete caret se utiliza para configurar los parámetros de entrenamiento. El parámetro "method = cv" especifica que se usará la técnica de validación cruzada para evaluar el rendimiento del modelo. El número de "folds" o particiones en las que se dividirá el conjunto de datos para la validación cruzada se establece en "number = 10". El objeto "seeds" generado anteriormente se utiliza para asegurar la reproducibilidad del proceso de entrenamiento. "verboseIter = FALSE" desactiva la impresión de información detallada durante el proceso de entrenamiento, mientras que "returnResamp = final" indica que solo se devolverán los resultados finales del proceso de entrenamiento. Por último, "search = random" especifica que se utilizará una búsqueda aleatoria de hiperparámetros durante el proceso de ajuste del modelo.

```{r}
set.seed(456)
control <- trainControl(method = "cv",
                        number = 10,
                        seeds = seeds,
                        verboseIter = FALSE,
                        returnResamp = "final",
                        search = "random")
```

Para entrenar el modelo de clasificación utilizando el método de bosques aleatorios (random forest) implementado en la función ranger del paquete caret, se usan los datos de entrenamiento previamente seleccionados, representados por el objeto DAST_train_pred. Los hiperparámetros del modelo son ajustados utilizando una búsqueda aleatoria (search = "random") a lo largo de 30 iteraciones (tuneLength = 30) y se utiliza el objeto control previamente definido para la validación cruzada. Además, se especifica que se quiere evaluar la importancia de las variables utilizando la técnica de permutación (importance = "permutation") en el modelo entrenado.

```{r,eval=FALSE}
DAST_pred <- caret::train(resp.pCR ~ ., data = DAST_train_pred,
                      method = "ranger",
                      trControl = control,
                      tuneLength = 30,
                      importance = "permutation")
saveRDS(DAST_pred,"DAST_pred")
```

Visualización de los resultados del modelo.

```{r}
DAST_pred <- readRDS("/home/alumno25/ML/DAST_pred")
DAST_pred
```

La salida muestra los resultados de un proceso de ajuste de hiperparámetros para el algoritmo de Random Forest. En este se evaluaron diferentes combinaciones de valores para los hiperparámetros de "mtry" (número de predictores a considerar en cada división de un árbol), "splitrule" (regla de división de nodos) y "min.node.size" (tamaño mínimo de nodo). Para cada combinación de hiperparámetros, se muestra la precisión y el valor kappa obtenidos en la validación cruzada de 10-fold. El modelo seleccionado como óptimo fue el que tuvo el mayor valor de precisión, con "mtry" = 9, "splitrule" = gini y "min.node.size" = 14.

16. Evaluación de la capacidad predictiva del modelo.

```{r}
predicciones_DAST <- predict(DAST_pred, newdata = DAST_test_pred)
confusionMatrix(data = predicciones_DAST, reference = DAST_test_pred$resp.pCR, positive = "1")
```

En este caso, el modelo predijo correctamente 21 de las instancias que pertenecen a la clase 0 y las 4 instancias que pertenecen a la clase 1. También se predijeron incorrectamente 3 instancias de la clase 0 y no se predijeron instancias de la clase 0 incorrectamente.

La precisión global del modelo, medida por la exactitud (accuracy), es del 89.29%, lo que indica que el modelo clasifica correctamente la mayoría de las instancias en el conjunto de datos. Además, el coeficiente kappa de 0.6667 indica un acuerdo sustancial entre las predicciones del modelo y las observaciones reales. La sensibilidad del modelo es de 0.5714, lo que indica que el modelo es capaz de detectar correctamente el 57.14% de las instancias que pertenecen a la clase 1. La especificidad del modelo es de 1, lo que indica que el modelo es capaz de identificar correctamente el 100% de las instancias que pertenecen a la clase 0.

17. Se crea un dataframe para comparar los valores de las méticas del modelo del conjunto de datos DAST(original) y DAST_pred en el cual se ha hecho la eliminación recursiva de variables.

```{r}
tabla_rendimiento3 <- data.frame(
  Métrica = c("Accuracy", "Kappa", "Sensitivity", "Specificity"),
  DAST = c(0.8571, 0.5294, 0.4286, 1.0000),
  DAST_pred= c(0.8929,0.6667,0.5714,1.0000)
)

print(tabla_rendimiento3)
```

Los resultados de esta tabla muestran que el modelo DAST_pred tiene una capacidad predictiva mayor que el modelo DAST original, ya que tiene una precisión del 89.29% en comparación con el 85.71%. Además, el valor de Kappa también ha mejorado significativamente de 0.5294 a 0.6667, lo que indica una mayor concordancia entre las predicciones del modelo y las observaciones reales. La sensibilidad también ha mejorado ligeramente de 0.4286 a 0.5714, mientras que la especificidad se mantiene en un alto valor de 1.0.

18. Para comparar las métricos de los distintos modelos realizados con el conjunto de datos DAST se realiza la siguiente gráfica:

```{r}
tabla_rendimiento2_long <- pivot_longer(tabla_rendimiento3, -Métrica)
ggplot(tabla_rendimiento2_long, aes(x = Métrica, y = value, fill = name)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Métrica", y = "Valor", fill = "Modelo") +
  ggtitle("Comparación de rendimiento entre modelos") +
  theme_minimal()
```

De esta gráfica se puede interpretar que:

-   El **Modelo DAST** (original) tiene una precisión del 85,71%, lo que significa que el 85,71% de las predicciones son correctas. El valor de Kappa de 0,5294 indica que el modelo tiene un acuerdo moderado con las observaciones reales. La sensibilidad de 0,4286 indica que el modelo identifica correctamente el 42,86% de las muestras positivas, mientras que la especificidad de 1 indica que el modelo identifica correctamente el 100% de las muestras negativas.

-   El **Modelo DAST_reducido**, que utiliza la técnica varIMP para eliminar variables menos relevantes, tiene una precisión del 82,14%, lo que indica una pequeña disminución en la capacidad del modelo para predecir la variable objetivo en comparación con el modelo DAST original. El valor de Kappa de 0,5 indica un acuerdo moderado con las observaciones reales. La sensibilidad de 0,5714 indica que el modelo identifica correctamente el 57,14% de las muestras positivas, mientras que la especificidad de 0,9048 indica que el modelo identifica correctamente el 90,48% de las muestras negativas.

-   El **Modelo DAST_pred**, que utiliza la eliminación recursiva de variables, tiene la mayor precisión de los tres modelos, con un valor de 89,29%. El valor de Kappa de 0,6667 indica un acuerdo sustancial con las observaciones reales, es decir, el modelo tiene una buena capacidad para clasificar correctamente los casos en las categorías correspondientes. La sensibilidad de 0,5714 indica que el modelo identifica correctamente el 57,14% de las muestras positivas, mientras que la especificidad de 1 indica que el modelo identifica correctamente el 100% de las muestras negativas.

Por tanto, esto sugiere que la eliminación recursiva de variables ha mejorado la capacidad del modelo para predecir la variable objetivo en comparación con los otros dos modelos.

# Conclusiones del trabajo

Tras realizar diversas técnicas como modelos lineales generalizados (GLM), Random Forest y Redes Neuronales en los cuatro ficheros de datos clínicos, análisis digital, sistema inmune y transcriptómica, se concluye que el modelo más adecuado para determinar si la paciente ha respondido bien al tratamiento es el modelo de Random Forest.

En particular, se puede destacar la comparación que se ha hecho en el apartado 2.2.3 de la técnica Random Forest con la Red Neurnal.El modelo que ha usado Random Forest proporciona una precisión del 89,29% y una sensibilidad del 57,14%, lo que indica que es capaz de detectar correctamente a más de la mitad de los pacientes que necesitan cirugía. Aunque el modelo de Redes Neuronales optimizado también podría ser adecuado con una sensibilidad del 57%, es importante tener en cuenta que tiene una menor especificidad del 80% en comparación con el modelo de Random Forest que tiene una especificidad del 100%.Por lo tanto, aunque la opción de la Red Neuronal podría ser explorada más a fondo, se debe considerar cuidadosamente los beneficios y riesgos de cada modelo antes de tomar la decisión de cuál es el más adecuado para su aplicación clínica. En general, en función de todos los modelos realizados en esta práctica se llega a la conclusión que el modelo de Random Forest es el más adecuado en este estudio para maximizar la sensibilidad del modelo y detectar correctamente a la mayoría de los pacientes que necesitan cirugía.

Para visualizar gráficamente lo dicho anteriormente:

```{r}
tabla_rendimiento4 <- data.frame(
  Métrica = c("Accuracy", "Kappa", "Sensitivity", "Specificity"),
  GLMDC=c(0.75,0.3636,0.5714,0.8095),
  GLMAD=c(0.6429,-0.0526,0.14286,0.80952),
  GLMSI=c(0.67,0.1,0.28571,0.80952),
  GLMtrans= c(0.9643,0.90,0.85,1.00),
  RFDC=c(0.8214, 0.375, 0.28571, 1.0000),
  RFAD=c(0.75, 0.125, 0.14286, 0.95238),
  RFSI=c(0.6429, -0.0526, 0.14286, 0.80952),
  RFtrans=c(0.7857, 0.4286, 0.5714, 0.8571),
  RNDC=c(0.75,0.3636,0.5714,0.8095)
)

tabla_rendimiento4_long <- pivot_longer(tabla_rendimiento4, -Métrica)
ggplot(tabla_rendimiento4_long, aes(x = Métrica, y = value, fill = name)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Métrica", y = "Valor", fill = "Modelo") +
  ggtitle("Comparación de rendimiento entre modelos random forest") +
  theme_minimal()
```

Además, cabe destacar que como se puede observar también en la gráfica anterior,el modelo GLMtrans muestra una alta precisión y sensibilidad en la predicción de la variable objetivo.A pesar de esto, su falta de convergencia adecuada y la presencia de probabilidades ajustadas extremadamente cercanas a 0 o 1 plantea ciertas preocupaciones en cuanto a su fiabilidad. Debido a esto, se ha decidido no tomar en cuenta este modelo para la aplicación clínica. Es importante tener en cuenta que el uso de modelos estadísticos como GLMtrans puede ser útil en algunos casos, pero se debe considerar cuidadosamente su idoneidad en cada situación y los posibles efectos de su falta de convergencia. En general, los resultados de este estudio indican que el modelo de bosque aleatorio es el más adecuado para la predicción de la variable objetivo en esta población.

Al realizar las comparaciones de modelo de Random Forest en conjuntos de datos por separados y unidos se llega a la conclusión de que el mejor modelo fue el que combinaba la información de los cuatro ficheros, mediante la implementación de la técnica de eliminación recursiva de variables. Este modelo muestra una alta capacidad predictiva, con una precisión del 89,29%. Además, el valor de Kappa de 0,6667 indica un acuerdo sustancial entre las predicciones del modelo y las observaciones reales, lo que sugiere que el modelo es capaz de clasificar correctamente los casos en las categorías correspondientes. La sensibilidad del modelo, que es la capacidad para identificar correctamente las muestras positivas, fue del 57,14%, mientras que la especificidad, que es la capacidad para identificar correctamente las muestras negativas, fue del 100%. Esto sugiere que la combinación de información de los cuatro ficheros, junto con una selección de variables, es el mejor modelo para predecir la respuesta de cada paciente a la terapia neoadyuvante, específicamente la respuesta pCR.

Es importante destacar que la eliminación recursiva de variables utilizada en este modelo es una técnica de selección de características que reduce el número de variables utilizadas en el modelo, eliminando aquellas que tienen menos importancia para la predicción. De esta manera, se reduce el ruido y se mejora la precisión del modelo. La combinación de información de los cuatro ficheros, junto con la eliminación recursiva de variables, permite integrar diferentes aspectos de la biología del cáncer de mama y proporcionar una predicción más precisa de la respuesta a la terapia neoadyuvante. Este enfoque puede ser útil para el diseño de ensayos clínicos personalizados y para la toma de decisiones clínicas en el tratamiento del cáncer de mama.

Visualización de lo explicado anteriormente:

```{r}
tabla_rendimiento5 <- data.frame(
  Métrica = c("Accuracy", "Kappa", "Sensitivity", "Specificity"),
  DC=c(0.8214, 0.375, 0.28571, 1.0000),
  AD=c(0.75, 0.125, 0.14286, 0.95238),
  SI=c(0.6429, -0.0526, 0.14286, 0.80952),
  trans=c(0.7857, 0.4286, 0.5714, 0.8571),
  DAST = c(0.8571, 0.5294, 0.4286, 1.0000),
  DAST_pred= c(0.8929,0.6667,0.5714,1.0000)
)

tabla_rendimiento5_long <- pivot_longer(tabla_rendimiento5, -Métrica)
ggplot(tabla_rendimiento5_long, aes(x = Métrica, y = value, fill = name)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Métrica", y = "Valor", fill = "Modelo") +
  ggtitle("Comparación de rendimiento entre modelos random forest") +
  theme_minimal()
```

Para concluir cuales son las variables más relevantes en el modelo que se ha elegido previamente como óptimo se realiza el siguiente código:

```{r}
import<- varImp(DAST_pred)
print(import)
```

Al observar la tabla de variables importantes se interpreta que en un estudio de cáncer de mama, estas variables se consideran relevantes porque se sabe que están asociadas con la progresión del cáncer y pueden ser útiles para predecir la respuesta al tratamiento y el pronóstico del paciente.

ESR1 y PGR son receptores hormonales que están presentes en la mayoría de los tumores de mama y son importantes para determinar la terapia hormonal. El nivel de expresión de estos receptores puede ayudar a predecir la respuesta al tratamiento hormonal y el pronóstico del paciente.

La densidad linfocitaria y la fracción linfocitaria son indicadores de la respuesta inmune del cuerpo al cáncer. Un alto nivel de infiltración linfocitaria se ha relacionado con una mejor respuesta al tratamiento y un mejor pronóstico.

Los componentes principales (PC) corresponde con la cuantificación de genes relevantes en forma de un conteo de las reads obtenidas de los ensayos RNA-seq expresión génica, lo que puede ser útil para identificar genes que están implicados en la progresión del cáncer.Para visualizar el nombre de estos genes se realiza el siguiente código:

```{r}
genes_PC58 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC48"]),decreasing = TRUE)[1:20], ])
genes_PC58
```

```{r}
genes_PC58 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC48"]),decreasing = TRUE)[1:20], ])
genes_PC58
```

```{r}
genes_PC48 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC48"]),decreasing = TRUE)[1:20], ])
genes_PC48
```

```{r}
genes_PC19 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC48"]),decreasing = TRUE)[1:20], ])
genes_PC19
```

```{r}
genes_PC2 <- rownames(pca_loadings[order(abs(pca_loadings[, "PC48"]),decreasing = TRUE)[1:20], ])
genes_PC2
```

La puntuación ER Allred es un sistema de puntuación utilizado para evaluar la expresión de los receptores de estrógeno en los tumores de mama. Puntuaciones más altas se asocian con una mayor expresión de los receptores de estrógeno y una mejor respuesta al tratamiento hormonal.

Las células inmunitarias como las células NK (natural killer), mastocitos y neutrófilos pueden tener un papel importante en la respuesta inmune al cáncer y la progresión del tumor. Por lo tanto, su presencia y número pueden ser útiles para predecir la respuesta al tratamiento y el pronóstico.

En general, estas variables se consideran relevantes en un estudio de cáncer de mama porque están relacionadas con la biología del cáncer y pueden ayudar a predecir la respuesta al tratamiento y el pronóstico del paciente.

En el artículo se concluye que la unión de la información proporcionada por los distintos archivos contribuye a determinar mejor la respuesta al tratamiento, lo que coincide con el modelo considerado como el mejor en esta práctica, que es la combinación de los 4 ficheros. La lista de variables relevantes muestra que la edad, la expresión de PGR y ESR1, y la densidad linfocitaria son características importantes en el modelo de entrenamiento integrado. A pesar de que ERBB2 se considera una característica importante en otros estudios de cáncer de mama, no aparece en la lista de variables relevantes en este estudio en particular. Esto puede deberse a diferencias en los análisis utilizados.

Es importante tener en cuenta que el uso de modelos de aprendizaje automático para identificar características relevantes puede no incluir todas las características importantes, ya que estos modelos pueden estar limitados por las características y la calidad de los datos disponibles. Por lo tanto, se deben interpretar los resultados con precaución y considerar la posibilidad de que otras características importantes puedan haber sido pasadas por alto.

De manera general se concluye de esta práctica que la combinación de información de distintas fuentes puede mejorar la determinación de la respuesta al tratamiento en pacientes con cáncer de mama, pero los resultados deben ser interpretados con precaución y considerando las limitaciones de los modelos de aprendizaje automático.

# Referencias. 

1. Sammut, S.J., Crispin-Ortuzar, M., Chin, S.F., et al. (2022). Multi-omic machine learning predictor of breast cancer therapy response. Nature, 601(7892), 623-629. <https://doi.org/10.1038/s41586-021-04278-5>

2.  Organización Mundial de la Salud. (s.f.). Cáncer de mama. Recuperado el 28 de marzo de 2023, de <https://www.who.int/es/news-room/fact-sheets/detail/breast-cancer>

3.  Ciencia de Datos. (s.f.). Machine Learning con R. Recuperado el 15 de marzo de 2023, de <https://www.cienciadedatos.net/machine-learning-r.html>

4.  Fernández Casal, R., Costa Bouzas, J., & Oviedo de la Fuente, M. (s.f.). Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas. Recuperado el 22 de marzo de 2023, de <https://rubenfcasal.github.io/aprendizaje_estadistico/>
